{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eefcd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "import shap\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552320cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization settings\\\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958e3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and etc:\n",
      "- Target: injury_next_14_days\n",
      "- Random State: 42\n",
      "- Model Name: nba_injury_predictor_v1\n",
      "- TensorFlow version: 2.19.0\n",
      "- GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration and random seeds\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COLUMN = 'injury_next_14_days'\n",
    "MODEL_NAME = 'nba_injury_predictor_v1'\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Configuration and etc:\")\n",
    "print(f\"- Target: {TARGET_COLUMN}\")\n",
    "print(f\"- Random State: {RANDOM_STATE}\")\n",
    "print(f\"- Model Name: {MODEL_NAME}\")\n",
    "print(f\"- TensorFlow version: {tf.__version__}\")\n",
    "print(f\"- GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f29ab9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "- Training: (8850, 40) features, 8850 samples\n",
      "- Validation: (2567, 40) features, 2567 samples\n",
      "- Test: (589, 40) features, 589 samples\n",
      "- Selected features: 40\n",
      "- Class weights: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "- Preprocessing config loaded\n",
      "- Feature selection metadata loaded\n",
      "- Data split validation loaded\n",
      "- Feature consistency across all splits\n",
      "\n",
      "Target distribution validation:\n",
      "- Training positive rate: 23.1% (after SMOTE)\n",
      "- Validation positive rate: 3.0%\n",
      "- Test positive rate: 1.0%\n",
      "- No missing values in any split\n",
      "- All features are numeric\n",
      "\n",
      "All data validation checks passed!\n",
      "\n",
      "Feature Statistics (Training Data):\n",
      "- Mean range: -0.102 to 1124.981\n",
      "- Std range: 0.028 to 271.169\n",
      "- Min values: -11.500 to 20.074\n",
      "- Max values: 0.170 to 1757.000\n",
      "- Features with std > 10: 4 (may need scaling)\n",
      "\n",
      "Class Balance Check:\n",
      "- Training: {0: 6808, 1: 2042}\n",
      "- Validation: {0: 2490, 1: 77}\n",
      "- Test: {0: 583, 1: 6}\n",
      "\n",
      "Sample Features by Type:\n",
      "- Workload features (5): ['total_actions_7d', 'shooting_load_30d', 'actions_trend_7d']...\n",
      "- Fatigue features (4): ['rest_days_since_last', 'is_back_to_back', 'season_fatigue']...\n",
      "- Context features (2): ['contact_usage_rate', 'age_at_game']...\n",
      "- Features scaled using RobustScaler\n",
      "  - Training scaled shape: (8850, 40)\n",
      "  - Scaled feature stats: mean≈0.151, std≈1.235\n",
      "- Data converted to TensorFlow format\n",
      "  - Input shape: (8850, 40)\n",
      "  - Target shape: (8850,)\n",
      "  - Data types: float32, float32\n",
      "- Scaler saved for deployment\n",
      "Data loaded and prepared for modeling stage\n",
      "Prepared to build TensorFlow model with 40 features\n"
     ]
    }
   ],
   "source": [
    "# Loads all processed data\n",
    "# Training data (SMOTE balanced + feature selected)\n",
    "X_train = pd.read_csv('../data/processed/X_train_final.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train_final.csv').squeeze()\n",
    "\n",
    "# Validation data (feature selected)\n",
    "X_val = pd.read_csv('../data/processed/X_validation_final.csv')\n",
    "y_val = pd.read_csv('../data/processed/y_validation_final.csv').squeeze()\n",
    "\n",
    "# Test data (feature selected)\n",
    "X_test = pd.read_csv('../data/processed/X_test_final.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test_final.csv').squeeze()\n",
    "\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"- Training: {X_train.shape} features, {len(y_train)} samples\")\n",
    "print(f\"- Validation: {X_val.shape} features, {len(y_val)} samples\") \n",
    "print(f\"- Test: {X_test.shape} features, {len(y_test)} samples\")\n",
    "\n",
    "# Loads metadata and configuration\n",
    "# Selected features list\n",
    "selected_features = joblib.load('../data/processed/selected_features.pkl')\n",
    "print(f\"- Selected features: {len(selected_features)}\")\n",
    "\n",
    "# Class weights for handling imbalance\n",
    "class_weights = joblib.load('../data/processed/class_weights.pkl')\n",
    "print(f\"- Class weights: {class_weights}\")\n",
    "\n",
    "# Preprocessing configuration\n",
    "preprocessing_config = joblib.load('../data/processed/preprocessing_config.pkl')\n",
    "print(f\"- Preprocessing config loaded\")\n",
    "\n",
    "# Feature selection results\n",
    "feature_selection_results = joblib.load('../data/processed/feature_selection_results.pkl')\n",
    "print(f\"- Feature selection metadata loaded\")\n",
    "\n",
    "# Split information for validation\n",
    "split_info = joblib.load('../data/processed/split_info.pkl')\n",
    "print(f\"- Data split validation loaded\")\n",
    "\n",
    "# Data validation and consistency checks\n",
    "# Check feature consistency\n",
    "assert list(X_train.columns) == selected_features, \"Training features don't match selected features\"\n",
    "assert list(X_val.columns) == selected_features, \"Validation features don't match selected features\"  \n",
    "assert list(X_test.columns) == selected_features, \"Test features don't match selected features\"\n",
    "print(\"- Feature consistency across all splits\")\n",
    "\n",
    "# Checks target distributions\n",
    "train_positive_rate = y_train.mean()\n",
    "val_positive_rate = y_val.mean()\n",
    "test_positive_rate = y_test.mean()\n",
    "\n",
    "print(f\"\\nTarget distribution validation:\")\n",
    "print(f\"- Training positive rate: {train_positive_rate:.1%} (after SMOTE)\")\n",
    "print(f\"- Validation positive rate: {val_positive_rate:.1%}\")\n",
    "print(f\"- Test positive rate: {test_positive_rate:.1%}\")\n",
    "\n",
    "# Checks for missing values\n",
    "train_missing = X_train.isnull().sum().sum()\n",
    "val_missing = X_val.isnull().sum().sum()\n",
    "test_missing = X_test.isnull().sum().sum()\n",
    "\n",
    "assert train_missing == 0, f\"Training data has {train_missing} missing values\"\n",
    "assert val_missing == 0, f\"Validation data has {val_missing} missing values\"\n",
    "assert test_missing == 0, f\"Test data has {test_missing} missing values\"\n",
    "print(\"- No missing values in any split\")\n",
    "\n",
    "# Verifies data types\n",
    "assert X_train.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in training\"\n",
    "assert X_val.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in validation\"\n",
    "assert X_test.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in test\"\n",
    "print(\"- All features are numeric\")\n",
    "\n",
    "print(\"\\nAll data validation checks passed!\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nFeature Statistics (Training Data):\")\n",
    "print(f\"- Mean range: {X_train.mean().min():.3f} to {X_train.mean().max():.3f}\")\n",
    "print(f\"- Std range: {X_train.std().min():.3f} to {X_train.std().max():.3f}\")\n",
    "print(f\"- Min values: {X_train.min().min():.3f} to {X_train.min().max():.3f}\")\n",
    "print(f\"- Max values: {X_train.max().min():.3f} to {X_train.max().max():.3f}\")\n",
    "\n",
    "# Checks for potential scaling issues\n",
    "features_need_scaling = (X_train.std() > 10).sum()\n",
    "print(f\"- Features with std > 10: {features_need_scaling} (may need scaling)\")\n",
    "\n",
    "# Targets class balance verification\n",
    "print(f\"\\nClass Balance Check:\")\n",
    "print(f\"- Training: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"- Validation: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"- Test: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Sample feature names by category\n",
    "print(f\"\\nSample Features by Type:\")\n",
    "workload_features = [f for f in selected_features if any(x in f for x in ['_7d', '_30d', 'load'])]\n",
    "fatigue_features = [f for f in selected_features if any(x in f for x in ['fatigue', 'rest', 'back_to_back'])]\n",
    "context_features = [f for f in selected_features if any(x in f for x in ['age', 'bmi', 'position'])]\n",
    "\n",
    "print(f\"- Workload features ({len(workload_features)}): {workload_features[:3]}...\")\n",
    "print(f\"- Fatigue features ({len(fatigue_features)}): {fatigue_features[:3]}...\")\n",
    "print(f\"- Context features ({len(context_features)}): {context_features[:3]}...\")\n",
    "\n",
    "\n",
    "# Data preprocessing for modeling\n",
    "# Feature scaling\n",
    "# RobustScaler to handle outliers better than StandardScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=selected_features, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test.index)\n",
    "\n",
    "print(f\"- Features scaled using RobustScaler\")\n",
    "print(f\"  - Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  - Scaled feature stats: mean≈{X_train_scaled.mean().mean():.3f}, std≈{X_train_scaled.std().mean():.3f}\")\n",
    "\n",
    "# Converts to numpy arrays for TensorFlow\n",
    "X_train_tf = X_train_scaled.values.astype(np.float32)\n",
    "X_val_tf = X_val_scaled.values.astype(np.float32)\n",
    "X_test_tf = X_test_scaled.values.astype(np.float32)\n",
    "y_train_tf = y_train.values.astype(np.float32)\n",
    "y_val_tf = y_val.values.astype(np.float32)\n",
    "y_test_tf = y_test.values.astype(np.float32)\n",
    "\n",
    "print(f\"- Data converted to TensorFlow format\")\n",
    "print(f\"  - Input shape: {X_train_tf.shape}\")\n",
    "print(f\"  - Target shape: {y_train_tf.shape}\")\n",
    "print(f\"  - Data types: {X_train_tf.dtype}, {y_train_tf.dtype}\")\n",
    "\n",
    "# Stores scaler for later use\n",
    "joblib.dump(scaler, f'../data/processed/{MODEL_NAME}_scaler.pkl')\n",
    "print(f\"- Scaler saved for deployment\")\n",
    "\n",
    "print(\"Data loaded and prepared for modeling stage\")\n",
    "print(f\"Prepared to build TensorFlow model with {X_train_tf.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05676021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes after all preprocessing:\n",
      "  - X_train: (8850, 40)\n",
      "  - y_train: (8850,)\n",
      "  - X_val: (2567, 40)\n",
      "  - y_val: (2567,)\n",
      "  - X_test: (589, 40)\n",
      "  - y_test: (589,)\n",
      "\n",
      "Class distribution summary:\n",
      "  - Training: [6808 2042] (ratio: 3.3:1)\n",
      "  - Validation: [2490   77] (ratio: 32.3:1)\n",
      "  - Test: [583   6] (ratio: 97.2:1)\n",
      "\n",
      "Class weights for model: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "\n",
      "Ready\n",
      "   - Features: 40\n",
      "   - Training samples: 8,850\n",
      "   - Target: injury_next_14_days\n",
      "   - Model: nba_injury_predictor_v1\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "print(\"Data shapes after all preprocessing:\")\n",
    "print(f\"  - X_train: {X_train_tf.shape}\")\n",
    "print(f\"  - y_train: {y_train_tf.shape}\")\n",
    "print(f\"  - X_val: {X_val_tf.shape}\")\n",
    "print(f\"  - y_val: {y_val_tf.shape}\")\n",
    "print(f\"  - X_test: {X_test_tf.shape}\")\n",
    "print(f\"  - y_test: {y_test_tf.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution summary:\")\n",
    "print(f\"  - Training: {np.bincount(y_train_tf.astype(int))} (ratio: {(y_train_tf == 0).sum()/(y_train_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Validation: {np.bincount(y_val_tf.astype(int))} (ratio: {(y_val_tf == 0).sum()/(y_val_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Test: {np.bincount(y_test_tf.astype(int))} (ratio: {(y_test_tf == 0).sum()/(y_test_tf == 1).sum():.1f}:1)\")\n",
    "\n",
    "print(f\"\\nClass weights for model: {class_weights}\")\n",
    "\n",
    "print(f\"\\nReady\")\n",
    "print(f\"   - Features: {X_train_tf.shape[1]}\")\n",
    "print(f\"   - Training samples: {X_train_tf.shape[0]:,}\")\n",
    "print(f\"   - Target: {TARGET_COLUMN}\")\n",
    "print(f\"   - Model: {MODEL_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
