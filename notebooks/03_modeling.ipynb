{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eefcd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "import shap\n",
    "from datetime import datetime\n",
    "\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import time\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552320cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization settings\\\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958e3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and etc:\n",
      "- Target: injury_next_14_days\n",
      "- Random State: 42\n",
      "- Model Name: nba_injury_predictor_v1\n",
      "- TensorFlow version: 2.19.0\n",
      "- GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration and random seeds\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COLUMN = 'injury_next_14_days'\n",
    "MODEL_NAME = 'nba_injury_predictor_v1'\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Configuration and etc:\")\n",
    "print(f\"- Target: {TARGET_COLUMN}\")\n",
    "print(f\"- Random State: {RANDOM_STATE}\")\n",
    "print(f\"- Model Name: {MODEL_NAME}\")\n",
    "print(f\"- TensorFlow version: {tf.__version__}\")\n",
    "print(f\"- GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29ab9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "- Training: (8850, 40) features, 8850 samples\n",
      "- Validation: (2567, 40) features, 2567 samples\n",
      "- Test: (589, 40) features, 589 samples\n",
      "- Selected features: 40\n",
      "- Class weights: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "- Preprocessing config loaded\n",
      "- Feature selection metadata loaded\n",
      "- Data split validation loaded\n",
      "- Feature consistency across all splits\n",
      "\n",
      "Target distribution validation:\n",
      "- Training positive rate: 23.1% (after SMOTE)\n",
      "- Validation positive rate: 3.0%\n",
      "- Test positive rate: 1.0%\n",
      "- No missing values in any split\n",
      "- All features are numeric\n",
      "\n",
      "All data validation checks passed!\n",
      "\n",
      "Feature Statistics (Training Data):\n",
      "- Mean range: -0.102 to 1124.981\n",
      "- Std range: 0.028 to 271.169\n",
      "- Min values: -11.500 to 20.074\n",
      "- Max values: 0.170 to 1757.000\n",
      "- Features with std > 10: 4 (may need scaling)\n",
      "\n",
      "Class Balance Check:\n",
      "- Training: {0: 6808, 1: 2042}\n",
      "- Validation: {0: 2490, 1: 77}\n",
      "- Test: {0: 583, 1: 6}\n",
      "\n",
      "Sample Features by Type:\n",
      "- Workload features (5): ['total_actions_7d', 'shooting_load_30d', 'actions_trend_7d']...\n",
      "- Fatigue features (4): ['rest_days_since_last', 'is_back_to_back', 'season_fatigue']...\n",
      "- Context features (2): ['contact_usage_rate', 'age_at_game']...\n",
      "- Features scaled using RobustScaler\n",
      "  - Training scaled shape: (8850, 40)\n",
      "  - Scaled feature stats: mean≈0.151, std≈1.235\n",
      "- Data converted to TensorFlow format\n",
      "  - Input shape: (8850, 40)\n",
      "  - Target shape: (8850,)\n",
      "  - Data types: float32, float32\n",
      "- Scaler saved for deployment\n",
      "Data loaded and prepared for modeling stage\n",
      "Prepared to build TensorFlow model with 40 features\n"
     ]
    }
   ],
   "source": [
    "# Loads all processed data\n",
    "# Training data (SMOTE balanced + feature selected)\n",
    "X_train = pd.read_csv('../data/processed/X_train_final.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train_final.csv').squeeze()\n",
    "\n",
    "# Validation data (feature selected)\n",
    "X_val = pd.read_csv('../data/processed/X_validation_final.csv')\n",
    "y_val = pd.read_csv('../data/processed/y_validation_final.csv').squeeze()\n",
    "\n",
    "# Test data (feature selected)\n",
    "X_test = pd.read_csv('../data/processed/X_test_final.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test_final.csv').squeeze()\n",
    "\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"- Training: {X_train.shape} features, {len(y_train)} samples\")\n",
    "print(f\"- Validation: {X_val.shape} features, {len(y_val)} samples\") \n",
    "print(f\"- Test: {X_test.shape} features, {len(y_test)} samples\")\n",
    "\n",
    "# Loads metadata and configuration\n",
    "# Selected features list\n",
    "selected_features = joblib.load('../data/processed/selected_features.pkl')\n",
    "print(f\"- Selected features: {len(selected_features)}\")\n",
    "\n",
    "# Class weights for handling imbalance\n",
    "class_weights = joblib.load('../data/processed/class_weights.pkl')\n",
    "print(f\"- Class weights: {class_weights}\")\n",
    "\n",
    "# Preprocessing configuration\n",
    "preprocessing_config = joblib.load('../data/processed/preprocessing_config.pkl')\n",
    "print(f\"- Preprocessing config loaded\")\n",
    "\n",
    "# Feature selection results\n",
    "feature_selection_results = joblib.load('../data/processed/feature_selection_results.pkl')\n",
    "print(f\"- Feature selection metadata loaded\")\n",
    "\n",
    "# Split information for validation\n",
    "split_info = joblib.load('../data/processed/split_info.pkl')\n",
    "print(f\"- Data split validation loaded\")\n",
    "\n",
    "# Data validation and consistency checks\n",
    "# Check feature consistency\n",
    "assert list(X_train.columns) == selected_features, \"Training features don't match selected features\"\n",
    "assert list(X_val.columns) == selected_features, \"Validation features don't match selected features\"  \n",
    "assert list(X_test.columns) == selected_features, \"Test features don't match selected features\"\n",
    "print(\"- Feature consistency across all splits\")\n",
    "\n",
    "# Checks target distributions\n",
    "train_positive_rate = y_train.mean()\n",
    "val_positive_rate = y_val.mean()\n",
    "test_positive_rate = y_test.mean()\n",
    "\n",
    "print(f\"\\nTarget distribution validation:\")\n",
    "print(f\"- Training positive rate: {train_positive_rate:.1%} (after SMOTE)\")\n",
    "print(f\"- Validation positive rate: {val_positive_rate:.1%}\")\n",
    "print(f\"- Test positive rate: {test_positive_rate:.1%}\")\n",
    "\n",
    "# Checks for missing values\n",
    "train_missing = X_train.isnull().sum().sum()\n",
    "val_missing = X_val.isnull().sum().sum()\n",
    "test_missing = X_test.isnull().sum().sum()\n",
    "\n",
    "assert train_missing == 0, f\"Training data has {train_missing} missing values\"\n",
    "assert val_missing == 0, f\"Validation data has {val_missing} missing values\"\n",
    "assert test_missing == 0, f\"Test data has {test_missing} missing values\"\n",
    "print(\"- No missing values in any split\")\n",
    "\n",
    "# Verifies data types\n",
    "assert X_train.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in training\"\n",
    "assert X_val.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in validation\"\n",
    "assert X_test.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in test\"\n",
    "print(\"- All features are numeric\")\n",
    "\n",
    "print(\"\\nAll data validation checks passed!\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nFeature Statistics (Training Data):\")\n",
    "print(f\"- Mean range: {X_train.mean().min():.3f} to {X_train.mean().max():.3f}\")\n",
    "print(f\"- Std range: {X_train.std().min():.3f} to {X_train.std().max():.3f}\")\n",
    "print(f\"- Min values: {X_train.min().min():.3f} to {X_train.min().max():.3f}\")\n",
    "print(f\"- Max values: {X_train.max().min():.3f} to {X_train.max().max():.3f}\")\n",
    "\n",
    "# Checks for potential scaling issues\n",
    "features_need_scaling = (X_train.std() > 10).sum()\n",
    "print(f\"- Features with std > 10: {features_need_scaling} (may need scaling)\")\n",
    "\n",
    "# Targets class balance verification\n",
    "print(f\"\\nClass Balance Check:\")\n",
    "print(f\"- Training: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"- Validation: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"- Test: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Sample feature names by category\n",
    "print(f\"\\nSample Features by Type:\")\n",
    "workload_features = [f for f in selected_features if any(x in f for x in ['_7d', '_30d', 'load'])]\n",
    "fatigue_features = [f for f in selected_features if any(x in f for x in ['fatigue', 'rest', 'back_to_back'])]\n",
    "context_features = [f for f in selected_features if any(x in f for x in ['age', 'bmi', 'position'])]\n",
    "\n",
    "print(f\"- Workload features ({len(workload_features)}): {workload_features[:3]}...\")\n",
    "print(f\"- Fatigue features ({len(fatigue_features)}): {fatigue_features[:3]}...\")\n",
    "print(f\"- Context features ({len(context_features)}): {context_features[:3]}...\")\n",
    "\n",
    "\n",
    "# Data preprocessing for modeling\n",
    "# Feature scaling\n",
    "# RobustScaler to handle outliers better than StandardScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=selected_features, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test.index)\n",
    "\n",
    "print(f\"- Features scaled using RobustScaler\")\n",
    "print(f\"  - Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  - Scaled feature stats: mean≈{X_train_scaled.mean().mean():.3f}, std≈{X_train_scaled.std().mean():.3f}\")\n",
    "\n",
    "# Converts to numpy arrays for TensorFlow\n",
    "X_train_tf = X_train_scaled.values.astype(np.float32)\n",
    "X_val_tf = X_val_scaled.values.astype(np.float32)\n",
    "X_test_tf = X_test_scaled.values.astype(np.float32)\n",
    "y_train_tf = y_train.values.astype(np.float32)\n",
    "y_val_tf = y_val.values.astype(np.float32)\n",
    "y_test_tf = y_test.values.astype(np.float32)\n",
    "\n",
    "print(f\"- Data converted to TensorFlow format\")\n",
    "print(f\"  - Input shape: {X_train_tf.shape}\")\n",
    "print(f\"  - Target shape: {y_train_tf.shape}\")\n",
    "print(f\"  - Data types: {X_train_tf.dtype}, {y_train_tf.dtype}\")\n",
    "\n",
    "# Stores scaler for later use\n",
    "joblib.dump(scaler, f'../data/processed/{MODEL_NAME}_scaler.pkl')\n",
    "print(f\"- Scaler saved for deployment\")\n",
    "\n",
    "print(\"Data loaded and prepared for modeling stage\")\n",
    "print(f\"Prepared to build TensorFlow model with {X_train_tf.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05676021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes after all preprocessing:\n",
      "  - X_train: (8850, 40)\n",
      "  - y_train: (8850,)\n",
      "  - X_val: (2567, 40)\n",
      "  - y_val: (2567,)\n",
      "  - X_test: (589, 40)\n",
      "  - y_test: (589,)\n",
      "\n",
      "Class distribution summary:\n",
      "  - Training: [6808 2042] (ratio: 3.3:1)\n",
      "  - Validation: [2490   77] (ratio: 32.3:1)\n",
      "  - Test: [583   6] (ratio: 97.2:1)\n",
      "\n",
      "Class weights for model: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "\n",
      "Ready\n",
      "   - Features: 40\n",
      "   - Training samples: 8,850\n",
      "   - Target: injury_next_14_days\n",
      "   - Model: nba_injury_predictor_v1\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "print(\"Data shapes after all preprocessing:\")\n",
    "print(f\"  - X_train: {X_train_tf.shape}\")\n",
    "print(f\"  - y_train: {y_train_tf.shape}\")\n",
    "print(f\"  - X_val: {X_val_tf.shape}\")\n",
    "print(f\"  - y_val: {y_val_tf.shape}\")\n",
    "print(f\"  - X_test: {X_test_tf.shape}\")\n",
    "print(f\"  - y_test: {y_test_tf.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution summary:\")\n",
    "print(f\"  - Training: {np.bincount(y_train_tf.astype(int))} (ratio: {(y_train_tf == 0).sum()/(y_train_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Validation: {np.bincount(y_val_tf.astype(int))} (ratio: {(y_val_tf == 0).sum()/(y_val_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Test: {np.bincount(y_test_tf.astype(int))} (ratio: {(y_test_tf == 0).sum()/(y_test_tf == 1).sum():.1f}:1)\")\n",
    "\n",
    "print(f\"\\nClass weights for model: {class_weights}\")\n",
    "\n",
    "print(f\"\\nReady\")\n",
    "print(f\"   - Features: {X_train_tf.shape[1]}\")\n",
    "print(f\"   - Training samples: {X_train_tf.shape[0]:,}\")\n",
    "print(f\"   - Target: {TARGET_COLUMN}\")\n",
    "print(f\"   - Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4232ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training scaled shape: (8850, 40)\n",
      "- Feature stats after scaling: mean=0.151, std=3.844\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"- Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"- Feature stats after scaling: mean={X_train_scaled.mean():.3f}, std={X_train_scaled.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41eb2c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING Performance:\n",
      "- Accuracy: 0.806\n",
      "- Precision: 0.544\n",
      "- Recall: 0.991\n",
      "- F1-Score: 0.702\n",
      "- ROC AUC: 0.982\n",
      "- PR AUC: 0.944\n",
      "Confusion Matrix:\n",
      "    TN: 5109 | FP: 1699\n",
      "    FN:   19 | TP: 2023\n",
      "\n",
      "VALIDATION Performance:\n",
      "- Accuracy: 0.676\n",
      "- Precision: 0.065\n",
      "- Recall: 0.727\n",
      "- F1-Score: 0.119\n",
      "- ROC AUC: 0.765\n",
      "- PR AUC: 0.182\n",
      "Confusion Matrix:\n",
      "    TN: 1679 | FP:  811\n",
      "    FN:   21 | TP:   56\n",
      "\n",
      "TEST Performance:\n",
      "- Accuracy: 0.506\n",
      "- Precision: 0.017\n",
      "- Recall: 0.833\n",
      "- F1-Score: 0.033\n",
      "- ROC AUC: 0.549\n",
      "- PR AUC: 0.013\n",
      "Confusion Matrix:\n",
      "    TN:  293 | FP:  290\n",
      "    FN:    1 | TP:    5\n"
     ]
    }
   ],
   "source": [
    "# Logisitc Regression\n",
    "logistic_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight=class_weights,  # Handle class imbalance\n",
    "    penalty='l2',                # L2 regularization\n",
    "    C=1.0,                      # Regularization strength (will tune later)\n",
    "    max_iter=1000,              # Ensure convergence\n",
    "    solver='lbfgs'              # Good for small datasets\n",
    ")\n",
    "\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "# Predictions on all sets\n",
    "y_train_pred = logistic_model.predict(X_train_scaled)\n",
    "y_val_pred = logistic_model.predict(X_val_scaled)\n",
    "y_test_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_prob = logistic_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_prob = logistic_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_prob = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "def evaluate_model_performance(y_true, y_pred, y_prob, dataset_name):\n",
    "    \"\"\"\n",
    "    Model evaluation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{dataset_name.upper()} Performance:\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # AUC metrics\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    print(f\"- Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"- Precision: {precision:.3f}\")\n",
    "    print(f\"- Recall: {recall:.3f}\")\n",
    "    print(f\"- F1-Score: {f1:.3f}\")\n",
    "    print(f\"- ROC AUC: {roc_auc:.3f}\")\n",
    "    print(f\"- PR AUC: {pr_auc:.3f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(f\"    TN: {cm[0,0]:4d} | FP: {cm[0,1]:4d}\")\n",
    "    print(f\"    FN: {cm[1,0]:4d} | TP: {cm[1,1]:4d}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc, 'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluates on all datasets\n",
    "train_metrics = evaluate_model_performance(y_train, y_train_pred, y_train_prob, \"Training\")\n",
    "val_metrics = evaluate_model_performance(y_val, y_val_pred, y_val_prob, \"Validation\")\n",
    "test_metrics = evaluate_model_performance(y_test, y_test_pred, y_test_prob, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e876b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  32.5% of injuries captured, precision =  19.5%\n",
      "    Top 10%:  45.5% of injuries captured, precision =  13.7%\n",
      "    Top 15%:  55.8% of injuries captured, precision =  11.2%\n",
      "    Top 20%:  59.7% of injuries captured, precision =   9.0%\n"
     ]
    }
   ],
   "source": [
    "# Top K Risk Prediciton Analysis\n",
    "\n",
    "def analyze_top_k_predictions(y_true, y_prob, k_values=[5, 10, 15, 20]):\n",
    "    \"\"\"\n",
    "    Analyzes what percentage of actual injuries are captured in top K% predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Top K Risk Prediction Performance:\")\n",
    "    \n",
    "    # Sorts by probability (highest risk first)\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    sorted_true = y_true[sorted_indices]\n",
    "    \n",
    "    total_positives = y_true.sum()\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Top k% of predictions\n",
    "        top_k_size = int(n_samples * k / 100)\n",
    "        top_k_true = sorted_true[:top_k_size]\n",
    "        \n",
    "        # Calculates capture rate\n",
    "        captured_positives = top_k_true.sum()\n",
    "        capture_rate = captured_positives / total_positives if total_positives > 0 else 0\n",
    "        precision_at_k = captured_positives / top_k_size if top_k_size > 0 else 0\n",
    "        \n",
    "        print(f\"    Top {k:2d}%: {capture_rate*100:5.1f}% of injuries captured, \"\n",
    "              f\"precision = {precision_at_k*100:5.1f}%\")\n",
    "\n",
    "# Analyzes on validation set\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50fafd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Most Important Features:\n",
      "Feature                   Coefficient  Impact         \n",
      "  season_fatigue                 7.406   Increase Injury Risk\n",
      "  missed_shots                  -6.904   Decrease Injury Risk\n",
      "  total_shot_attempts            5.936   Increase Injury Risk\n",
      "  is_late_season                -5.876   Decrease Injury Risk\n",
      "  is_mid_season                 -5.543   Decrease Injury Risk\n",
      "  made_shots                    -5.418   Decrease Injury Risk\n",
      "  rebounds                      -4.708   Decrease Injury Risk\n",
      "  total_actions_7d               4.109   Increase Injury Risk\n",
      "  actions_vs_career_avg          3.321   Increase Injury Risk\n",
      "  game_month                     3.138   Increase Injury Risk\n",
      "  is_early_season               -3.026   Decrease Injury Risk\n",
      "  current_vs_14day_avg           2.291   Increase Injury Risk\n",
      "  is_low_performance             1.630   Increase Injury Risk\n",
      "  substitutions                 -1.590   Decrease Injury Risk\n",
      "  total_season_games            -1.555   Decrease Injury Risk\n"
     ]
    }
   ],
   "source": [
    "# Feature importance Analysis\n",
    "\n",
    "# Gets feature coefficients (weights)\n",
    "feature_coefficients = logistic_model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'coefficient': feature_coefficients,\n",
    "    'abs_coefficient': np.abs(feature_coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"Top 15 Most Important Features:\")\n",
    "print(f\"{'Feature':<25} {'Coefficient':<12} {'Impact':<15}\")\n",
    "\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    impact = \"Increase Injury Risk\" if row['coefficient'] > 0 else \"Decrease Injury Risk\"\n",
    "    print(f\"  {row['feature']:<25} {row['coefficient']:>10.3f}   {impact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d968772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Training completed in 67.3 seconds\n",
      "Best parameters: {'class_weight': 'balanced_subsample', 'max_depth': 15, 'max_features': 0.5, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 150, 'random_state': 42}\n",
      "Best cross-validation PR-AUC: 0.9928\n",
      "\n",
      "TRAINING Performance:\n",
      "- Accuracy: 0.993\n",
      "- Precision: 0.979\n",
      "- Recall: 0.991\n",
      "- F1-Score: 0.985\n",
      "- ROC AUC: 1.000\n",
      "- PR AUC: 0.999\n",
      "Confusion Matrix:\n",
      "    TN: 6765 | FP:   43\n",
      "    FN:   18 | TP: 2024\n",
      "\n",
      "VALIDATION Performance:\n",
      "- Accuracy: 0.966\n",
      "- Precision: 0.452\n",
      "- Recall: 0.545\n",
      "- F1-Score: 0.494\n",
      "- ROC AUC: 0.832\n",
      "- PR AUC: 0.586\n",
      "Confusion Matrix:\n",
      "    TN: 2439 | FP:   51\n",
      "    FN:   35 | TP:   42\n",
      "\n",
      "TEST Performance:\n",
      "- Accuracy: 0.922\n",
      "- Precision: 0.000\n",
      "- Recall: 0.000\n",
      "- F1-Score: 0.000\n",
      "- ROC AUC: 0.491\n",
      "- PR AUC: 0.012\n",
      "Confusion Matrix:\n",
      "    TN:  543 | FP:   40\n",
      "    FN:    6 | TP:    0\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  59.7% of injuries captured, precision =  35.9%\n",
      "    Top 10%:  68.8% of injuries captured, precision =  20.7%\n",
      "    Top 15%:  71.4% of injuries captured, precision =  14.3%\n",
      "    Top 20%:  71.4% of injuries captured, precision =  10.7%\n",
      "Top 15 Most Important Features (Random Forest):\n",
      "Feature                   Importance  \n",
      "season_fatigue                0.4107\n",
      "is_mid_season                 0.0901\n",
      "total_season_games            0.0893\n",
      "is_late_season                0.0814\n",
      "is_early_season               0.0554\n",
      "game_month                    0.0399\n",
      "is_weekend_game               0.0270\n",
      "current_vs_14day_avg          0.0217\n",
      "is_low_performance            0.0184\n",
      "game_day_of_week              0.0159\n",
      "is_playoff_push               0.0150\n",
      "shooting_load_30d             0.0128\n",
      "actions_vs_career_avg         0.0116\n",
      "rest_days_since_last          0.0088\n",
      "age_at_game                   0.0085\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "\n",
    "# Hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 150], \n",
    "    'max_depth': [5, 10, 15],      \n",
    "    'min_samples_split': [20, 50, 100], \n",
    "    'min_samples_leaf': [5, 10, 20],    \n",
    "    'max_features': ['sqrt', 0.5],      \n",
    "    'class_weight': ['balanced_subsample'],\n",
    "    'random_state': [RANDOM_STATE]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Grid Search with Cross Validation\n",
    "cv_folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=cv_folds,\n",
    "    scoring='average_precision',  # PR-AUC for imbalanced data\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time:.1f} seconds\")\n",
    "print(f\"Best parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation PR-AUC: {rf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on all sets\n",
    "y_train_pred_rf = rf_model.predict(X_train_scaled)\n",
    "y_val_pred_rf = rf_model.predict(X_val_scaled)\n",
    "y_test_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_prob_rf = rf_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_prob_rf = rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluations\n",
    "train_metrics_rf = evaluate_model_performance(y_train, y_train_pred_rf, y_train_prob_rf, \"Training\")\n",
    "val_metrics_rf = evaluate_model_performance(y_val, y_val_pred_rf, y_val_prob_rf, \"Validation\")\n",
    "test_metrics_rf = evaluate_model_performance(y_test, y_test_pred_rf, y_test_prob_rf, \"Test\")\n",
    "\n",
    "# Top K Risk Analysis\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob_rf)\n",
    "\n",
    "# Feature Importance Analysis\n",
    "\n",
    "# Gets feature importances\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features (Random Forest):\")\n",
    "print(f\"{'Feature':<25} {'Importance':<12}\")\n",
    "\n",
    "for idx, row in feature_importance_rf.head(15).iterrows():\n",
    "    print(f\"{row['feature']:<25} {row['importance']:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8339b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training:\n",
      "- Negative class: 6,808\n",
      "- Positive class: 2,042\n",
      "- Scale pos weight: 3.33\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 3 folds for each of 2187 candidates, totalling 6561 fits\n",
      "\n",
      "Best hyperparameters:\n",
      "- colsample_bytree: 0.7\n",
      "- learning_rate: 0.1\n",
      "- max_depth: 7\n",
      "- n_estimators: 100\n",
      "- reg_alpha: 0.1\n",
      "- reg_lambda: 1\n",
      "- subsample: 0.8\n",
      "Best CV PR-AUC: 0.988\n",
      "\n",
      "XGBOOST TRAINING Performance:\n",
      "- Accuracy: 0.980\n",
      "- Precision: 0.939\n",
      "- Recall: 0.977\n",
      "- F1-Score: 0.958\n",
      "- ROC AUC: 0.997\n",
      "- PR AUC: 0.990\n",
      "Confusion Matrix:\n",
      "    TN: 6679 | FP:  129\n",
      "    FN:   47 | TP: 1995\n",
      "\n",
      "XGBOOST VALIDATION Performance:\n",
      "- Accuracy: 0.915\n",
      "- Precision: 0.207\n",
      "- Recall: 0.649\n",
      "- F1-Score: 0.314\n",
      "- ROC AUC: 0.839\n",
      "- PR AUC: 0.596\n",
      "Confusion Matrix:\n",
      "    TN: 2299 | FP:  191\n",
      "    FN:   27 | TP:   50\n",
      "\n",
      "XGBOOST TEST Performance:\n",
      "- Accuracy: 0.893\n",
      "- Precision: 0.000\n",
      "- Recall: 0.000\n",
      "- F1-Score: 0.000\n",
      "- ROC AUC: 0.539\n",
      "- PR AUC: 0.012\n",
      "Confusion Matrix:\n",
      "    TN:  526 | FP:   57\n",
      "    FN:    6 | TP:    0\n",
      "Top 20 Most Important Features (XGBoost):\n",
      "Feature                        Importance  \n",
      "is_late_season                     0.2126\n",
      "season_fatigue                     0.1542\n",
      "is_early_season                    0.1076\n",
      "is_mid_season                      0.0572\n",
      "game_month                         0.0455\n",
      "is_weekend_game                    0.0419\n",
      "is_low_performance                 0.0410\n",
      "is_playoff_push                    0.0342\n",
      "total_season_games                 0.0332\n",
      "is_short_rest                      0.0212\n",
      "is_dense_schedule                  0.0211\n",
      "game_day_of_week                   0.0205\n",
      "games_last_7_days                  0.0196\n",
      "current_vs_14day_avg               0.0182\n",
      "actions_vs_career_avg              0.0122\n",
      "games_last_6_days                  0.0120\n",
      "rest_days_since_last               0.0114\n",
      "shooting_load_30d                  0.0112\n",
      "rebounds                           0.0096\n",
      "free_throws                        0.0084\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  62.3% of injuries captured, precision =  37.5%\n",
      "    Top 10%:  64.9% of injuries captured, precision =  19.5%\n",
      "    Top 15%:  67.5% of injuries captured, precision =  13.5%\n",
      "    Top 20%:  68.8% of injuries captured, precision =  10.3%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model\n",
    "\n",
    "# XGBoost Model \n",
    "def train_xgboost_model(X_train_scaled, y_train, X_val_scaled, y_val, class_weights, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains XGBoost model\n",
    "    \"\"\"\n",
    "    # Calculates scale_pos_weight for XGBoost\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "    \n",
    "    print(f\"Class distribution in training:\")\n",
    "    print(f\"- Negative class: {neg_count:,}\")\n",
    "    print(f\"- Positive class: {pos_count:,}\")\n",
    "    print(f\"- Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Base XGBoost model \n",
    "    xgb_base = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=scale_pos_weight,  # Handles imbalance\n",
    "        random_state=random_state,\n",
    "        eval_metric=['logloss', 'auc'],\n",
    "        early_stopping_rounds=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1], \n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'reg_alpha': [0, 0.1, 0.5],  # L1 regularization\n",
    "        'reg_lambda': [1, 1.5, 2]   # L2 regularization\n",
    "    }\n",
    "    \n",
    "    # Stratified CV for imbalanced data\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Grid search with PR-AUC scoring\n",
    "    print(\"Performing hyperparameter tuning...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_base,\n",
    "        param_grid=param_grid,\n",
    "        scoring='average_precision',  # PR-AUC \n",
    "        cv=cv_strategy,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit with validation set for early stopping\n",
    "    grid_search.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"- {param}: {value}\")\n",
    "    \n",
    "    print(f\"Best CV PR-AUC: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return best_model, grid_search\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model, xgb_grid_search = train_xgboost_model(\n",
    "    X_train_scaled, y_train, \n",
    "    X_val_scaled, y_val, \n",
    "    class_weights\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "\n",
    "# Training predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_scaled)\n",
    "y_train_prob_xgb = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Validation predictions  \n",
    "y_val_pred_xgb = xgb_model.predict(X_val_scaled)\n",
    "y_val_prob_xgb = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_xgb = xgb_model.predict(X_test_scaled) \n",
    "y_test_prob_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluations\n",
    "xgb_train_metrics = evaluate_model_performance(y_train, y_train_pred_xgb, y_train_prob_xgb, \"XGBoost Training\")\n",
    "xgb_val_metrics = evaluate_model_performance(y_val, y_val_pred_xgb, y_val_prob_xgb, \"XGBoost Validation\") \n",
    "xgb_test_metrics = evaluate_model_performance(y_test, y_test_pred_xgb, y_test_prob_xgb, \"XGBoost Test\")\n",
    "\n",
    "# Feature importance analysis\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features (XGBoost):\")\n",
    "print(f\"{'Feature':<30} {'Importance':<12}\")\n",
    "for idx, row in feature_importance_xgb.head(20).iterrows():\n",
    "    print(f\"{row['feature']:<30} {row['importance']:>10.4f}\")\n",
    "\n",
    "# Top K Risk Analysis for XGBoost\n",
    "# Analyze on validation set\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob_xgb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
