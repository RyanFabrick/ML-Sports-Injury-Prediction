{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eefcd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "import shap\n",
    "from datetime import datetime\n",
    "\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import time\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "552320cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization settings\\\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "958e3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and etc:\n",
      "- Target: injury_next_14_days\n",
      "- Random State: 42\n",
      "- Model Name: nba_injury_predictor_v1\n",
      "- TensorFlow version: 2.19.0\n",
      "- GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration and random seeds\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COLUMN = 'injury_next_14_days'\n",
    "MODEL_NAME = 'nba_injury_predictor_v1'\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Configuration and etc:\")\n",
    "print(f\"- Target: {TARGET_COLUMN}\")\n",
    "print(f\"- Random State: {RANDOM_STATE}\")\n",
    "print(f\"- Model Name: {MODEL_NAME}\")\n",
    "print(f\"- TensorFlow version: {tf.__version__}\")\n",
    "print(f\"- GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f29ab9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "- Training: (6975, 34) features, 6975 samples\n",
      "- Validation: (2567, 34) features, 2567 samples\n",
      "- Test: (589, 34) features, 589 samples\n",
      "- Selected features: 34\n",
      "- Class weights: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "- Preprocessing config loaded\n",
      "- Feature selection metadata loaded\n",
      "- Data split validation loaded\n",
      "- Feature consistency across all splits\n",
      "\n",
      "Target distribution validation:\n",
      "- Training positive rate: 2.4% (after SMOTE)\n",
      "- Validation positive rate: 3.0%\n",
      "- Test positive rate: 1.0%\n",
      "- No missing values in any split\n",
      "- All features are numeric\n",
      "\n",
      "All data validation checks passed!\n",
      "\n",
      "Feature Statistics (Training Data):\n",
      "- Mean range: -0.000 to 1117.278\n",
      "- Std range: 0.029 to 280.307\n",
      "- Min values: -11.500 to 23.750\n",
      "- Max values: 0.170 to 1757.000\n",
      "- Features with std > 10: 5 (may need scaling)\n",
      "\n",
      "Class Balance Check:\n",
      "- Training: {0: 6808, 1: 167}\n",
      "- Validation: {0: 2490, 1: 77}\n",
      "- Test: {0: 583, 1: 6}\n",
      "\n",
      "Sample Features by Type:\n",
      "- Workload features (7): ['total_actions_30d', 'shooting_load_30d', 'defensive_load_30d']...\n",
      "- Fatigue features (3): ['rest_days_since_last', 'is_back_to_back', 'fatigue_score']...\n",
      "- Context features (3): ['contact_usage_rate', 'bmi', 'age_at_game']...\n",
      "- Features scaled using RobustScaler\n",
      "  - Training scaled shape: (6975, 34)\n",
      "  - Scaled feature stats: mean≈0.147, std≈1.505\n",
      "- Data converted to TensorFlow format\n",
      "  - Input shape: (6975, 34)\n",
      "  - Target shape: (6975,)\n",
      "  - Data types: float32, float32\n",
      "- Scaler saved for deployment\n",
      "Data loaded and prepared for modeling stage\n",
      "Prepared to build TensorFlow model with 34 features\n"
     ]
    }
   ],
   "source": [
    "# Loads all processed data\n",
    "# Training data (SMOTE balanced + feature selected)\n",
    "X_train = pd.read_csv('../data/processed/X_train_final.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train_final.csv').squeeze()\n",
    "\n",
    "# Validation data (feature selected)\n",
    "X_val = pd.read_csv('../data/processed/X_validation_final.csv')\n",
    "y_val = pd.read_csv('../data/processed/y_validation_final.csv').squeeze()\n",
    "\n",
    "# Test data (feature selected)\n",
    "X_test = pd.read_csv('../data/processed/X_test_final.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test_final.csv').squeeze()\n",
    "\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"- Training: {X_train.shape} features, {len(y_train)} samples\")\n",
    "print(f\"- Validation: {X_val.shape} features, {len(y_val)} samples\") \n",
    "print(f\"- Test: {X_test.shape} features, {len(y_test)} samples\")\n",
    "\n",
    "# Loads metadata and configuration\n",
    "# Selected features list\n",
    "selected_features = joblib.load('../data/processed/selected_features.pkl')\n",
    "print(f\"- Selected features: {len(selected_features)}\")\n",
    "\n",
    "# Class weights for handling imbalance\n",
    "class_weights = joblib.load('../data/processed/class_weights.pkl')\n",
    "print(f\"- Class weights: {class_weights}\")\n",
    "\n",
    "# Preprocessing configuration\n",
    "preprocessing_config = joblib.load('../data/processed/preprocessing_config.pkl')\n",
    "print(f\"- Preprocessing config loaded\")\n",
    "\n",
    "# Feature selection results\n",
    "feature_selection_results = joblib.load('../data/processed/feature_selection_results.pkl')\n",
    "print(f\"- Feature selection metadata loaded\")\n",
    "\n",
    "# Split information for validation\n",
    "split_info = joblib.load('../data/processed/split_info.pkl')\n",
    "print(f\"- Data split validation loaded\")\n",
    "\n",
    "# Data validation and consistency checks\n",
    "# Check feature consistency\n",
    "assert list(X_train.columns) == selected_features, \"Training features don't match selected features\"\n",
    "assert list(X_val.columns) == selected_features, \"Validation features don't match selected features\"  \n",
    "assert list(X_test.columns) == selected_features, \"Test features don't match selected features\"\n",
    "print(\"- Feature consistency across all splits\")\n",
    "\n",
    "# Checks target distributions\n",
    "train_positive_rate = y_train.mean()\n",
    "val_positive_rate = y_val.mean()\n",
    "test_positive_rate = y_test.mean()\n",
    "\n",
    "print(f\"\\nTarget distribution validation:\")\n",
    "print(f\"- Training positive rate: {train_positive_rate:.1%} (after SMOTE)\")\n",
    "print(f\"- Validation positive rate: {val_positive_rate:.1%}\")\n",
    "print(f\"- Test positive rate: {test_positive_rate:.1%}\")\n",
    "\n",
    "# Checks for missing values\n",
    "train_missing = X_train.isnull().sum().sum()\n",
    "val_missing = X_val.isnull().sum().sum()\n",
    "test_missing = X_test.isnull().sum().sum()\n",
    "\n",
    "assert train_missing == 0, f\"Training data has {train_missing} missing values\"\n",
    "assert val_missing == 0, f\"Validation data has {val_missing} missing values\"\n",
    "assert test_missing == 0, f\"Test data has {test_missing} missing values\"\n",
    "print(\"- No missing values in any split\")\n",
    "\n",
    "# Verifies data types\n",
    "assert X_train.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in training\"\n",
    "assert X_val.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in validation\"\n",
    "assert X_test.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in test\"\n",
    "print(\"- All features are numeric\")\n",
    "\n",
    "print(\"\\nAll data validation checks passed!\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nFeature Statistics (Training Data):\")\n",
    "print(f\"- Mean range: {X_train.mean().min():.3f} to {X_train.mean().max():.3f}\")\n",
    "print(f\"- Std range: {X_train.std().min():.3f} to {X_train.std().max():.3f}\")\n",
    "print(f\"- Min values: {X_train.min().min():.3f} to {X_train.min().max():.3f}\")\n",
    "print(f\"- Max values: {X_train.max().min():.3f} to {X_train.max().max():.3f}\")\n",
    "\n",
    "# Checks for potential scaling issues\n",
    "features_need_scaling = (X_train.std() > 10).sum()\n",
    "print(f\"- Features with std > 10: {features_need_scaling} (may need scaling)\")\n",
    "\n",
    "# Targets class balance verification\n",
    "print(f\"\\nClass Balance Check:\")\n",
    "print(f\"- Training: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"- Validation: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"- Test: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Sample feature names by category\n",
    "print(f\"\\nSample Features by Type:\")\n",
    "workload_features = [f for f in selected_features if any(x in f for x in ['_7d', '_30d', 'load'])]\n",
    "fatigue_features = [f for f in selected_features if any(x in f for x in ['fatigue', 'rest', 'back_to_back'])]\n",
    "context_features = [f for f in selected_features if any(x in f for x in ['age', 'bmi', 'position'])]\n",
    "\n",
    "print(f\"- Workload features ({len(workload_features)}): {workload_features[:3]}...\")\n",
    "print(f\"- Fatigue features ({len(fatigue_features)}): {fatigue_features[:3]}...\")\n",
    "print(f\"- Context features ({len(context_features)}): {context_features[:3]}...\")\n",
    "\n",
    "\n",
    "# Data preprocessing for modeling\n",
    "# Feature scaling\n",
    "# RobustScaler to handle outliers better than StandardScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=selected_features, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test.index)\n",
    "\n",
    "print(f\"- Features scaled using RobustScaler\")\n",
    "print(f\"  - Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  - Scaled feature stats: mean≈{X_train_scaled.mean().mean():.3f}, std≈{X_train_scaled.std().mean():.3f}\")\n",
    "\n",
    "# Converts to numpy arrays for TensorFlow\n",
    "X_train_tf = X_train_scaled.values.astype(np.float32)\n",
    "X_val_tf = X_val_scaled.values.astype(np.float32)\n",
    "X_test_tf = X_test_scaled.values.astype(np.float32)\n",
    "y_train_tf = y_train.values.astype(np.float32)\n",
    "y_val_tf = y_val.values.astype(np.float32)\n",
    "y_test_tf = y_test.values.astype(np.float32)\n",
    "\n",
    "print(f\"- Data converted to TensorFlow format\")\n",
    "print(f\"  - Input shape: {X_train_tf.shape}\")\n",
    "print(f\"  - Target shape: {y_train_tf.shape}\")\n",
    "print(f\"  - Data types: {X_train_tf.dtype}, {y_train_tf.dtype}\")\n",
    "\n",
    "# Stores scaler for later use\n",
    "joblib.dump(scaler, f'../data/processed/{MODEL_NAME}_scaler.pkl')\n",
    "print(f\"- Scaler saved for deployment\")\n",
    "\n",
    "print(\"Data loaded and prepared for modeling stage\")\n",
    "print(f\"Prepared to build TensorFlow model with {X_train_tf.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05676021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes after all preprocessing:\n",
      "  - X_train: (6975, 34)\n",
      "  - y_train: (6975,)\n",
      "  - X_val: (2567, 34)\n",
      "  - y_val: (2567,)\n",
      "  - X_test: (589, 34)\n",
      "  - y_test: (589,)\n",
      "\n",
      "Class distribution summary:\n",
      "  - Training: [6808  167] (ratio: 40.8:1)\n",
      "  - Validation: [2490   77] (ratio: 32.3:1)\n",
      "  - Test: [583   6] (ratio: 97.2:1)\n",
      "\n",
      "Class weights for model: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "\n",
      "Ready\n",
      "   - Features: 34\n",
      "   - Training samples: 6,975\n",
      "   - Target: injury_next_14_days\n",
      "   - Model: nba_injury_predictor_v1\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "print(\"Data shapes after all preprocessing:\")\n",
    "print(f\"  - X_train: {X_train_tf.shape}\")\n",
    "print(f\"  - y_train: {y_train_tf.shape}\")\n",
    "print(f\"  - X_val: {X_val_tf.shape}\")\n",
    "print(f\"  - y_val: {y_val_tf.shape}\")\n",
    "print(f\"  - X_test: {X_test_tf.shape}\")\n",
    "print(f\"  - y_test: {y_test_tf.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution summary:\")\n",
    "print(f\"  - Training: {np.bincount(y_train_tf.astype(int))} (ratio: {(y_train_tf == 0).sum()/(y_train_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Validation: {np.bincount(y_val_tf.astype(int))} (ratio: {(y_val_tf == 0).sum()/(y_val_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Test: {np.bincount(y_test_tf.astype(int))} (ratio: {(y_test_tf == 0).sum()/(y_test_tf == 1).sum():.1f}:1)\")\n",
    "\n",
    "print(f\"\\nClass weights for model: {class_weights}\")\n",
    "\n",
    "print(f\"\\nReady\")\n",
    "print(f\"   - Features: {X_train_tf.shape[1]}\")\n",
    "print(f\"   - Training samples: {X_train_tf.shape[0]:,}\")\n",
    "print(f\"   - Target: {TARGET_COLUMN}\")\n",
    "print(f\"   - Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4232ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training scaled shape: (6975, 34)\n",
      "- Feature stats after scaling: mean=0.147, std=4.644\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"- Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"- Feature stats after scaling: mean={X_train_scaled.mean():.3f}, std={X_train_scaled.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41eb2c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING Performance:\n",
      "- Accuracy: 0.747\n",
      "- Precision: 0.067\n",
      "- Recall: 0.743\n",
      "- F1-Score: 0.123\n",
      "- ROC AUC: 0.830\n",
      "- PR AUC: 0.185\n",
      "Confusion Matrix:\n",
      "    TN: 5088 | FP: 1720\n",
      "    FN:   43 | TP:  124\n",
      "\n",
      "VALIDATION Performance:\n",
      "- Accuracy: 0.757\n",
      "- Precision: 0.057\n",
      "- Recall: 0.455\n",
      "- F1-Score: 0.101\n",
      "- ROC AUC: 0.645\n",
      "- PR AUC: 0.098\n",
      "Confusion Matrix:\n",
      "    TN: 1907 | FP:  583\n",
      "    FN:   42 | TP:   35\n",
      "\n",
      "TEST Performance:\n",
      "- Accuracy: 0.477\n",
      "- Precision: 0.007\n",
      "- Recall: 0.333\n",
      "- F1-Score: 0.013\n",
      "- ROC AUC: 0.372\n",
      "- PR AUC: 0.009\n",
      "Confusion Matrix:\n",
      "    TN:  279 | FP:  304\n",
      "    FN:    4 | TP:    2\n"
     ]
    }
   ],
   "source": [
    "# Logisitc Regression\n",
    "logistic_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight=class_weights,  # Handle class imbalance\n",
    "    penalty='l2',                # L2 regularization\n",
    "    C=1.0,                      # Regularization strength (will tune later)\n",
    "    max_iter=1000,              # Ensure convergence\n",
    "    solver='lbfgs'              # Good for small datasets\n",
    ")\n",
    "\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "# Predictions on all sets\n",
    "y_train_pred = logistic_model.predict(X_train_scaled)\n",
    "y_val_pred = logistic_model.predict(X_val_scaled)\n",
    "y_test_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_prob = logistic_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_prob = logistic_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_prob = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "def evaluate_model_performance(y_true, y_pred, y_prob, dataset_name):\n",
    "    \"\"\"\n",
    "    Model evaluation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{dataset_name.upper()} Performance:\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # AUC metrics\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    print(f\"- Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"- Precision: {precision:.3f}\")\n",
    "    print(f\"- Recall: {recall:.3f}\")\n",
    "    print(f\"- F1-Score: {f1:.3f}\")\n",
    "    print(f\"- ROC AUC: {roc_auc:.3f}\")\n",
    "    print(f\"- PR AUC: {pr_auc:.3f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(f\"    TN: {cm[0,0]:4d} | FP: {cm[0,1]:4d}\")\n",
    "    print(f\"    FN: {cm[1,0]:4d} | TP: {cm[1,1]:4d}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc, 'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluates on all datasets\n",
    "train_metrics = evaluate_model_performance(y_train, y_train_pred, y_train_prob, \"Training\")\n",
    "val_metrics = evaluate_model_performance(y_val, y_val_pred, y_val_prob, \"Validation\")\n",
    "test_metrics = evaluate_model_performance(y_test, y_test_pred, y_test_prob, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e876b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  14.3% of injuries captured, precision =   8.6%\n",
      "    Top 10%:  27.3% of injuries captured, precision =   8.2%\n",
      "    Top 15%:  32.5% of injuries captured, precision =   6.5%\n",
      "    Top 20%:  37.7% of injuries captured, precision =   5.7%\n"
     ]
    }
   ],
   "source": [
    "# Top K Risk Prediciton Analysis\n",
    "\n",
    "def analyze_top_k_predictions(y_true, y_prob, k_values=[5, 10, 15, 20]):\n",
    "    \"\"\"\n",
    "    Analyzes what percentage of actual injuries are captured in top K% predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Top K Risk Prediction Performance:\")\n",
    "    \n",
    "    # Sorts by probability (highest risk first)\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    sorted_true = y_true[sorted_indices]\n",
    "    \n",
    "    total_positives = y_true.sum()\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Top k% of predictions\n",
    "        top_k_size = int(n_samples * k / 100)\n",
    "        top_k_true = sorted_true[:top_k_size]\n",
    "        \n",
    "        # Calculates capture rate\n",
    "        captured_positives = top_k_true.sum()\n",
    "        capture_rate = captured_positives / total_positives if total_positives > 0 else 0\n",
    "        precision_at_k = captured_positives / top_k_size if top_k_size > 0 else 0\n",
    "        \n",
    "        print(f\"    Top {k:2d}%: {capture_rate*100:5.1f}% of injuries captured, \"\n",
    "              f\"precision = {precision_at_k*100:5.1f}%\")\n",
    "\n",
    "# Analyzes on validation set\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50fafd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Most Important Features:\n",
      "Feature                   Coefficient  Impact         \n",
      "  is_back_to_back               -1.844   Decrease Injury Risk\n",
      "  fatigue_score                  1.818   Increase Injury Risk\n",
      "  cumulative_actions_30d        -1.467   Decrease Injury Risk\n",
      "  total_actions                  1.417   Increase Injury Risk\n",
      "  shooting_load_30d              1.175   Increase Injury Risk\n",
      "  is_low_performance             1.132   Increase Injury Risk\n",
      "  rebounds                      -0.978   Decrease Injury Risk\n",
      "  missed_shots                  -0.951   Decrease Injury Risk\n",
      "  defensive_load_30d             0.905   Increase Injury Risk\n",
      "  total_shot_attempts           -0.608   Decrease Injury Risk\n",
      "  game_day_of_week              -0.502   Decrease Injury Risk\n",
      "  total_actions_30d              0.437   Increase Injury Risk\n",
      "  shooting_efficiency           -0.403   Decrease Injury Risk\n",
      "  contact_usage_rate            -0.376   Decrease Injury Risk\n",
      "  is_weekend_game               -0.369   Decrease Injury Risk\n"
     ]
    }
   ],
   "source": [
    "# Feature importance Analysis\n",
    "\n",
    "# Gets feature coefficients (weights)\n",
    "feature_coefficients = logistic_model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'coefficient': feature_coefficients,\n",
    "    'abs_coefficient': np.abs(feature_coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"Top 15 Most Important Features:\")\n",
    "print(f\"{'Feature':<25} {'Coefficient':<12} {'Impact':<15}\")\n",
    "\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    impact = \"Increase Injury Risk\" if row['coefficient'] > 0 else \"Decrease Injury Risk\"\n",
    "    print(f\"  {row['feature']:<25} {row['coefficient']:>10.3f}   {impact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d968772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Training completed in 76.0 seconds\n",
      "Best parameters: {'class_weight': 'balanced_subsample', 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 20, 'min_samples_split': 50, 'n_estimators': 150, 'random_state': 42}\n",
      "Best cross-validation PR-AUC: 0.1729\n",
      "\n",
      "TRAINING Performance:\n",
      "- Accuracy: 0.985\n",
      "- Precision: 0.624\n",
      "- Recall: 0.964\n",
      "- F1-Score: 0.758\n",
      "- ROC AUC: 0.996\n",
      "- PR AUC: 0.812\n",
      "Confusion Matrix:\n",
      "    TN: 6711 | FP:   97\n",
      "    FN:    6 | TP:  161\n",
      "\n",
      "VALIDATION Performance:\n",
      "- Accuracy: 0.961\n",
      "- Precision: 0.239\n",
      "- Recall: 0.143\n",
      "- F1-Score: 0.179\n",
      "- ROC AUC: 0.647\n",
      "- PR AUC: 0.121\n",
      "Confusion Matrix:\n",
      "    TN: 2455 | FP:   35\n",
      "    FN:   66 | TP:   11\n",
      "\n",
      "TEST Performance:\n",
      "- Accuracy: 0.947\n",
      "- Precision: 0.037\n",
      "- Recall: 0.167\n",
      "- F1-Score: 0.061\n",
      "- ROC AUC: 0.356\n",
      "- PR AUC: 0.015\n",
      "Confusion Matrix:\n",
      "    TN:  557 | FP:   26\n",
      "    FN:    5 | TP:    1\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  20.8% of injuries captured, precision =  12.5%\n",
      "    Top 10%:  26.0% of injuries captured, precision =   7.8%\n",
      "    Top 15%:  33.8% of injuries captured, precision =   6.8%\n",
      "    Top 20%:  40.3% of injuries captured, precision =   6.0%\n",
      "Top 15 Most Important Features (Random Forest):\n",
      "Feature                   Importance  \n",
      "fatigue_score                 0.1538\n",
      "game_day_of_week              0.0658\n",
      "current_vs_14day_avg          0.0487\n",
      "performance_drop_7vs30        0.0482\n",
      "total_actions                 0.0457\n",
      "shots_vs_season_avg           0.0387\n",
      "efficiency_trend_7d           0.0358\n",
      "contact_usage_rate            0.0349\n",
      "actions_trend_7d              0.0331\n",
      "rebounds_vs_season_avg        0.0325\n",
      "shooting_eff_decline          0.0304\n",
      "substitution_frequency        0.0300\n",
      "shooting_load_30d             0.0282\n",
      "defensive_load_30d            0.0277\n",
      "cumulative_actions_30d        0.0277\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "\n",
    "# Hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 150], \n",
    "    'max_depth': [5, 10, 15],      \n",
    "    'min_samples_split': [20, 50, 100], \n",
    "    'min_samples_leaf': [5, 10, 20],    \n",
    "    'max_features': ['sqrt', 0.5],      \n",
    "    'class_weight': ['balanced_subsample'],\n",
    "    'random_state': [RANDOM_STATE]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Grid Search with Cross Validation\n",
    "cv_folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=cv_folds,\n",
    "    scoring='average_precision',  # PR-AUC for imbalanced data\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time:.1f} seconds\")\n",
    "print(f\"Best parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation PR-AUC: {rf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on all sets\n",
    "y_train_pred_rf = rf_model.predict(X_train_scaled)\n",
    "y_val_pred_rf = rf_model.predict(X_val_scaled)\n",
    "y_test_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_prob_rf = rf_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_prob_rf = rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluations\n",
    "train_metrics_rf = evaluate_model_performance(y_train, y_train_pred_rf, y_train_prob_rf, \"Training\")\n",
    "val_metrics_rf = evaluate_model_performance(y_val, y_val_pred_rf, y_val_prob_rf, \"Validation\")\n",
    "test_metrics_rf = evaluate_model_performance(y_test, y_test_pred_rf, y_test_prob_rf, \"Test\")\n",
    "\n",
    "# Top K Risk Analysis\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob_rf)\n",
    "\n",
    "# Feature Importance Analysis\n",
    "\n",
    "# Gets feature importances\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features (Random Forest):\")\n",
    "print(f\"{'Feature':<25} {'Importance':<12}\")\n",
    "\n",
    "for idx, row in feature_importance_rf.head(15).iterrows():\n",
    "    print(f\"{row['feature']:<25} {row['importance']:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8339b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training:\n",
      "- Negative class: 6,808\n",
      "- Positive class: 167\n",
      "- Scale pos weight: 40.77\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 3 folds for each of 2187 candidates, totalling 6561 fits\n",
      "\n",
      "Best hyperparameters:\n",
      "- colsample_bytree: 0.7\n",
      "- learning_rate: 0.1\n",
      "- max_depth: 3\n",
      "- n_estimators: 100\n",
      "- reg_alpha: 0.5\n",
      "- reg_lambda: 2\n",
      "- subsample: 0.7\n",
      "Best CV PR-AUC: 0.143\n",
      "\n",
      "XGBOOST TRAINING Performance:\n",
      "- Accuracy: 0.822\n",
      "- Precision: 0.097\n",
      "- Recall: 0.772\n",
      "- F1-Score: 0.172\n",
      "- ROC AUC: 0.884\n",
      "- PR AUC: 0.261\n",
      "Confusion Matrix:\n",
      "    TN: 5607 | FP: 1201\n",
      "    FN:   38 | TP:  129\n",
      "\n",
      "XGBOOST VALIDATION Performance:\n",
      "- Accuracy: 0.851\n",
      "- Precision: 0.075\n",
      "- Recall: 0.351\n",
      "- F1-Score: 0.124\n",
      "- ROC AUC: 0.690\n",
      "- PR AUC: 0.083\n",
      "Confusion Matrix:\n",
      "    TN: 2158 | FP:  332\n",
      "    FN:   50 | TP:   27\n",
      "\n",
      "XGBOOST TEST Performance:\n",
      "- Accuracy: 0.727\n",
      "- Precision: 0.006\n",
      "- Recall: 0.167\n",
      "- F1-Score: 0.012\n",
      "- ROC AUC: 0.396\n",
      "- PR AUC: 0.011\n",
      "Confusion Matrix:\n",
      "    TN:  427 | FP:  156\n",
      "    FN:    5 | TP:    1\n",
      "Top 20 Most Important Features (XGBoost):\n",
      "Feature                        Importance  \n",
      "fatigue_score                      0.0806\n",
      "rebounds_vs_season_avg             0.0611\n",
      "current_vs_14day_avg               0.0526\n",
      "substitution_frequency             0.0520\n",
      "game_day_of_week                   0.0480\n",
      "shots_vs_season_avg                0.0434\n",
      "missed_shots                       0.0426\n",
      "is_back_to_back                    0.0420\n",
      "rest_days_since_last               0.0394\n",
      "defensive_load_30d                 0.0380\n",
      "shooting_eff_decline               0.0349\n",
      "contact_usage_rate                 0.0347\n",
      "fouls                              0.0337\n",
      "total_actions                      0.0332\n",
      "efficiency_trend_7d                0.0327\n",
      "shooting_load_30d                  0.0322\n",
      "bmi                                0.0299\n",
      "actions_trend_7d                   0.0275\n",
      "performance_drop_7vs30             0.0261\n",
      "turnovers                          0.0254\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  14.3% of injuries captured, precision =   8.6%\n",
      "    Top 10%:  23.4% of injuries captured, precision =   7.0%\n",
      "    Top 15%:  39.0% of injuries captured, precision =   7.8%\n",
      "    Top 20%:  51.9% of injuries captured, precision =   7.8%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model\n",
    "\n",
    "# XGBoost Model \n",
    "def train_xgboost_model(X_train_scaled, y_train, X_val_scaled, y_val, class_weights, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains XGBoost model\n",
    "    \"\"\"\n",
    "    # Calculates scale_pos_weight for XGBoost\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "    \n",
    "    print(f\"Class distribution in training:\")\n",
    "    print(f\"- Negative class: {neg_count:,}\")\n",
    "    print(f\"- Positive class: {pos_count:,}\")\n",
    "    print(f\"- Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Base XGBoost model \n",
    "    xgb_base = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=scale_pos_weight,  # Handles imbalance\n",
    "        random_state=random_state,\n",
    "        eval_metric=['logloss', 'auc'],\n",
    "        early_stopping_rounds=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1], \n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'reg_alpha': [0, 0.1, 0.5],  # L1 regularization\n",
    "        'reg_lambda': [1, 1.5, 2]   # L2 regularization\n",
    "    }\n",
    "    \n",
    "    # Stratified CV for imbalanced data\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Grid search with PR-AUC scoring\n",
    "    print(\"Performing hyperparameter tuning...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_base,\n",
    "        param_grid=param_grid,\n",
    "        scoring='average_precision',  # PR-AUC \n",
    "        cv=cv_strategy,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit with validation set for early stopping\n",
    "    grid_search.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"- {param}: {value}\")\n",
    "    \n",
    "    print(f\"Best CV PR-AUC: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return best_model, grid_search\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model, xgb_grid_search = train_xgboost_model(\n",
    "    X_train_scaled, y_train, \n",
    "    X_val_scaled, y_val, \n",
    "    class_weights\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "\n",
    "# Training predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_scaled)\n",
    "y_train_prob_xgb = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Validation predictions  \n",
    "y_val_pred_xgb = xgb_model.predict(X_val_scaled)\n",
    "y_val_prob_xgb = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_xgb = xgb_model.predict(X_test_scaled) \n",
    "y_test_prob_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluations\n",
    "xgb_train_metrics = evaluate_model_performance(y_train, y_train_pred_xgb, y_train_prob_xgb, \"XGBoost Training\")\n",
    "xgb_val_metrics = evaluate_model_performance(y_val, y_val_pred_xgb, y_val_prob_xgb, \"XGBoost Validation\") \n",
    "xgb_test_metrics = evaluate_model_performance(y_test, y_test_pred_xgb, y_test_prob_xgb, \"XGBoost Test\")\n",
    "\n",
    "# Feature importance analysis\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features (XGBoost):\")\n",
    "print(f\"{'Feature':<30} {'Importance':<12}\")\n",
    "for idx, row in feature_importance_xgb.head(20).iterrows():\n",
    "    print(f\"{row['feature']:<30} {row['importance']:>10.4f}\")\n",
    "\n",
    "# Top K Risk Analysis for XGBoost\n",
    "# Analyze on validation set\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e12b2a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neural Network training...\n",
      "Input features: 34\n",
      "Training samples: 6,975\n",
      "Class weights: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "Sample weights created. Shape: (6975,)\n",
      "Sample weight distribution: (array([ 0.51226498, 20.88323353]), array([6808,  167], dtype=int64))\n",
      "\n",
      "Neural Network Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_1                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_2                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_3                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_1                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_2                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_3 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_3                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,313</span> (20.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,313\u001b[0m (20.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,089</span> (19.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,089\u001b[0m (19.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration:\n",
      "- Batch size: 32\n",
      "- Max epochs: 200\n",
      "- Early stopping patience: 15\n",
      "- Learning rate reduction patience: 10\n",
      "\n",
      "Training Started...\n",
      "Epoch 1/200\n",
      "\u001b[1m210/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.6004 - accuracy: 0.4522 - loss: 0.7947 - precision: 0.0345 - recall: 0.6842\n",
      "Epoch 1: val_loss improved from None to 0.77382, saving model to ../data/processed/nba_injury_predictor_v1_best_nn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.5565 - accuracy: 0.4592 - loss: 0.7867 - precision: 0.0273 - recall: 0.6228 - val_AUC: 0.5138 - val_accuracy: 0.1434 - val_loss: 0.7738 - val_precision: 0.0322 - val_recall: 0.9481 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m195/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.6218 - accuracy: 0.4864 - loss: 0.7404 - precision: 0.0375 - recall: 0.7037\n",
      "Epoch 2: val_loss improved from 0.77382 to 0.77040, saving model to ../data/processed/nba_injury_predictor_v1_best_nn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6130 - accuracy: 0.5029 - loss: 0.7039 - precision: 0.0328 - recall: 0.6946 - val_AUC: 0.5568 - val_accuracy: 0.2926 - val_loss: 0.7704 - val_precision: 0.0323 - val_recall: 0.7792 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m208/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.6480 - accuracy: 0.5208 - loss: 0.7166 - precision: 0.0378 - recall: 0.6662\n",
      "Epoch 3: val_loss improved from 0.77040 to 0.76432, saving model to ../data/processed/nba_injury_predictor_v1_best_nn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6299 - accuracy: 0.5223 - loss: 0.6967 - precision: 0.0325 - recall: 0.6587 - val_AUC: 0.5799 - val_accuracy: 0.3958 - val_loss: 0.7643 - val_precision: 0.0335 - val_recall: 0.6883 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m206/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.6641 - accuracy: 0.5350 - loss: 0.6886 - precision: 0.0415 - recall: 0.7191\n",
      "Epoch 4: val_loss did not improve from 0.76432\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6496 - accuracy: 0.5411 - loss: 0.6600 - precision: 0.0358 - recall: 0.7006 - val_AUC: 0.5882 - val_accuracy: 0.4733 - val_loss: 0.7675 - val_precision: 0.0323 - val_recall: 0.5714 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m202/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.6907 - accuracy: 0.5569 - loss: 0.6813 - precision: 0.0468 - recall: 0.7822\n",
      "Epoch 5: val_loss improved from 0.76432 to 0.76034, saving model to ../data/processed/nba_injury_predictor_v1_best_nn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6839 - accuracy: 0.5685 - loss: 0.6443 - precision: 0.0395 - recall: 0.7305 - val_AUC: 0.6023 - val_accuracy: 0.5006 - val_loss: 0.7603 - val_precision: 0.0347 - val_recall: 0.5844 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m195/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.6815 - accuracy: 0.5678 - loss: 0.7016 - precision: 0.0405 - recall: 0.6460\n",
      "Epoch 6: val_loss improved from 0.76034 to 0.75123, saving model to ../data/processed/nba_injury_predictor_v1_best_nn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6911 - accuracy: 0.5723 - loss: 0.6423 - precision: 0.0371 - recall: 0.6766 - val_AUC: 0.6283 - val_accuracy: 0.5855 - val_loss: 0.7512 - val_precision: 0.0392 - val_recall: 0.5455 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m199/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.7466 - accuracy: 0.6038 - loss: 0.6268 - precision: 0.0554 - recall: 0.8224\n",
      "Epoch 7: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7170 - accuracy: 0.6044 - loss: 0.6212 - precision: 0.0440 - recall: 0.7485 - val_AUC: 0.6262 - val_accuracy: 0.5960 - val_loss: 0.7592 - val_precision: 0.0411 - val_recall: 0.5584 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m179/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.7472 - accuracy: 0.6111 - loss: 0.6379 - precision: 0.0529 - recall: 0.7656\n",
      "Epoch 8: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7677 - accuracy: 0.6221 - loss: 0.5762 - precision: 0.0476 - recall: 0.7784 - val_AUC: 0.6306 - val_accuracy: 0.6591 - val_loss: 0.7678 - val_precision: 0.0456 - val_recall: 0.5195 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m211/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.7648 - accuracy: 0.6327 - loss: 0.6121 - precision: 0.0541 - recall: 0.7484\n",
      "Epoch 9: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7682 - accuracy: 0.6406 - loss: 0.5704 - precision: 0.0465 - recall: 0.7186 - val_AUC: 0.6207 - val_accuracy: 0.7004 - val_loss: 0.7882 - val_precision: 0.0471 - val_recall: 0.4675 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m199/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7773 - accuracy: 0.6531 - loss: 0.5979 - precision: 0.0605 - recall: 0.7729\n",
      "Epoch 10: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7751 - accuracy: 0.6492 - loss: 0.5626 - precision: 0.0505 - recall: 0.7665 - val_AUC: 0.6243 - val_accuracy: 0.6385 - val_loss: 0.7794 - val_precision: 0.0439 - val_recall: 0.5325 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m172/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.7742 - accuracy: 0.6539 - loss: 0.6210 - precision: 0.0586 - recall: 0.7369\n",
      "Epoch 11: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7653 - accuracy: 0.6479 - loss: 0.5833 - precision: 0.0489 - recall: 0.7425 - val_AUC: 0.6218 - val_accuracy: 0.6584 - val_loss: 0.7978 - val_precision: 0.0434 - val_recall: 0.4935 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m201/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8091 - accuracy: 0.6707 - loss: 0.5777 - precision: 0.0638 - recall: 0.7973\n",
      "Epoch 12: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7932 - accuracy: 0.6734 - loss: 0.5459 - precision: 0.0541 - recall: 0.7665 - val_AUC: 0.6407 - val_accuracy: 0.6697 - val_loss: 0.7842 - val_precision: 0.0470 - val_recall: 0.5195 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m187/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.7939 - accuracy: 0.6703 - loss: 0.6066 - precision: 0.0656 - recall: 0.8134\n",
      "Epoch 13: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7957 - accuracy: 0.6803 - loss: 0.5538 - precision: 0.0579 - recall: 0.8084 - val_AUC: 0.6256 - val_accuracy: 0.7043 - val_loss: 0.8103 - val_precision: 0.0465 - val_recall: 0.4545 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m215/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8115 - accuracy: 0.6849 - loss: 0.5623 - precision: 0.0627 - recall: 0.7533\n",
      "Epoch 14: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8204 - accuracy: 0.6869 - loss: 0.5182 - precision: 0.0571 - recall: 0.7784 - val_AUC: 0.6277 - val_accuracy: 0.6810 - val_loss: 0.8047 - val_precision: 0.0420 - val_recall: 0.4416 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m211/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8097 - accuracy: 0.7040 - loss: 0.5729 - precision: 0.0694 - recall: 0.7816\n",
      "Epoch 15: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8133 - accuracy: 0.6981 - loss: 0.5247 - precision: 0.0591 - recall: 0.7784 - val_AUC: 0.6262 - val_accuracy: 0.7211 - val_loss: 0.8320 - val_precision: 0.0468 - val_recall: 0.4286 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m201/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8357 - accuracy: 0.7066 - loss: 0.5408 - precision: 0.0749 - recall: 0.8515\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8415 - accuracy: 0.7105 - loss: 0.4887 - precision: 0.0665 - recall: 0.8503 - val_AUC: 0.6371 - val_accuracy: 0.7324 - val_loss: 0.8449 - val_precision: 0.0488 - val_recall: 0.4286 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m206/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8220 - accuracy: 0.7157 - loss: 0.5534 - precision: 0.0734 - recall: 0.8026\n",
      "Epoch 17: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8380 - accuracy: 0.7167 - loss: 0.4930 - precision: 0.0654 - recall: 0.8144 - val_AUC: 0.6396 - val_accuracy: 0.7168 - val_loss: 0.8396 - val_precision: 0.0474 - val_recall: 0.4416 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m197/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8404 - accuracy: 0.7165 - loss: 0.5197 - precision: 0.0779 - recall: 0.8489\n",
      "Epoch 18: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8272 - accuracy: 0.7137 - loss: 0.5007 - precision: 0.0647 - recall: 0.8144 - val_AUC: 0.6356 - val_accuracy: 0.6919 - val_loss: 0.8447 - val_precision: 0.0458 - val_recall: 0.4675 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m215/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8415 - accuracy: 0.7146 - loss: 0.5273 - precision: 0.0745 - recall: 0.8263\n",
      "Epoch 19: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8434 - accuracy: 0.7137 - loss: 0.4901 - precision: 0.0655 - recall: 0.8263 - val_AUC: 0.6373 - val_accuracy: 0.7016 - val_loss: 0.8432 - val_precision: 0.0473 - val_recall: 0.4675 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m216/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8428 - accuracy: 0.7273 - loss: 0.5131 - precision: 0.0790 - recall: 0.8361\n",
      "Epoch 20: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8468 - accuracy: 0.7217 - loss: 0.4773 - precision: 0.0673 - recall: 0.8263 - val_AUC: 0.6394 - val_accuracy: 0.7055 - val_loss: 0.8600 - val_precision: 0.0467 - val_recall: 0.4545 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m199/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.8335 - accuracy: 0.7151 - loss: 0.5304 - precision: 0.0766 - recall: 0.8383\n",
      "Epoch 21: val_loss did not improve from 0.75123\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8287 - accuracy: 0.7168 - loss: 0.5025 - precision: 0.0645 - recall: 0.8024 - val_AUC: 0.6379 - val_accuracy: 0.7195 - val_loss: 0.8734 - val_precision: 0.0478 - val_recall: 0.4416 - learning_rate: 5.0000e-04\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Training completed in 11.9 seconds (0.2 minutes)\n",
      "\n",
      "NEURAL NETWORK TRAINING Performance:\n",
      "- Accuracy: 0.587\n",
      "- Precision: 0.047\n",
      "- Recall: 0.838\n",
      "- F1-Score: 0.089\n",
      "- ROC AUC: 0.797\n",
      "- PR AUC: 0.112\n",
      "Confusion Matrix:\n",
      "    TN: 3953 | FP: 2855\n",
      "    FN:   27 | TP:  140\n",
      "\n",
      "NEURAL NETWORK VALIDATION Performance:\n",
      "- Accuracy: 0.586\n",
      "- Precision: 0.039\n",
      "- Recall: 0.545\n",
      "- F1-Score: 0.073\n",
      "- ROC AUC: 0.628\n",
      "- PR AUC: 0.064\n",
      "Confusion Matrix:\n",
      "    TN: 1461 | FP: 1029\n",
      "    FN:   35 | TP:   42\n",
      "\n",
      "NEURAL NETWORK TEST Performance:\n",
      "- Accuracy: 0.428\n",
      "- Precision: 0.009\n",
      "- Recall: 0.500\n",
      "- F1-Score: 0.017\n",
      "- ROC AUC: 0.439\n",
      "- PR AUC: 0.013\n",
      "Confusion Matrix:\n",
      "    TN:  249 | FP:  334\n",
      "    FN:    3 | TP:    3\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  18.2% of injuries captured, precision =  10.9%\n",
      "    Top 10%:  27.3% of injuries captured, precision =   8.2%\n",
      "    Top 15%:  33.8% of injuries captured, precision =   6.8%\n",
      "    Top 20%:  37.7% of injuries captured, precision =   5.7%\n",
      "Training History Summary:\n",
      "- Total epochs trained: 21\n",
      "- Best epoch (lowest val_loss): 6\n",
      "- Final training loss: 0.5025\n",
      "- Final validation loss: 0.8734\n",
      "- Best validation loss: 0.7512\n",
      "\n",
      "Model Complexity:\n",
      "- Total parameters: 5,313\n",
      "- Trainable parameters: 5,089\n",
      "- Model size: ~20.8 KB (float32)\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Architecture\n",
    "def create_neural_network(input_dim, class_weights):\n",
    "    \"\"\"\n",
    "    Creates neural network architecture\n",
    "    - Input: 40 features  \n",
    "    - Hidden Layer 1: 64 units, ReLU, Dropout(0.3)\n",
    "    - Hidden Layer 2: 32 units, ReLU, Dropout(0.3) \n",
    "    - Hidden Layer 3: 16 units, ReLU, Dropout(0.2)\n",
    "    - Output: 1 unit, Sigmoid\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,), name='hidden_1'),\n",
    "        Dropout(0.3, name='dropout_1'),\n",
    "        BatchNormalization(name='batch_norm_1'),\n",
    "        \n",
    "        # 2nd hidden layer\n",
    "        Dense(32, activation='relu', name='hidden_2'),\n",
    "        Dropout(0.3, name='dropout_2'),\n",
    "        BatchNormalization(name='batch_norm_2'),\n",
    "        \n",
    "        # 3rd hidden layer  \n",
    "        Dense(16, activation='relu', name='hidden_3'),\n",
    "        Dropout(0.2, name='dropout_3'),\n",
    "        BatchNormalization(name='batch_norm_3'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    # Compiles w/ class weights incorporated into loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall', 'AUC']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Convert class weights to sample weights for TensorFlow\n",
    "def create_sample_weights(y, class_weights):\n",
    "    \"\"\"Convert class weights to sample weights for TensorFlow training\"\"\"\n",
    "    sample_weights = np.where(y == 1, class_weights[1], class_weights[0])\n",
    "    return sample_weights\n",
    "\n",
    "# Training setup\n",
    "print(\"Setting up Neural Network training...\")\n",
    "print(f\"Input features: {X_train_tf.shape[1]}\")\n",
    "print(f\"Training samples: {X_train_tf.shape[0]:,}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Creates sample weights for training\n",
    "train_sample_weights = create_sample_weights(y_train_tf, class_weights)\n",
    "val_sample_weights = create_sample_weights(y_val_tf, class_weights)\n",
    "\n",
    "print(f\"Sample weights created. Shape: {train_sample_weights.shape}\")\n",
    "print(f\"Sample weight distribution: {np.unique(train_sample_weights, return_counts=True)}\")\n",
    "\n",
    "# Builds the model\n",
    "nn_model = create_neural_network(X_train_tf.shape[1], class_weights)\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nNeural Network Architecture:\")\n",
    "nn_model.summary()\n",
    "\n",
    "# Callbacks for training\n",
    "callbacks_list = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'../data/processed/{MODEL_NAME}_best_nn.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "VALIDATION_SPLIT = 0.0  # Separate validation set\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")\n",
    "print(f\"- Max epochs: {EPOCHS}\")\n",
    "print(f\"- Early stopping patience: 15\")\n",
    "print(f\"- Learning rate reduction patience: 10\")\n",
    "\n",
    "# Trains the model\n",
    "print(\"\\nTraining Started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = nn_model.fit(\n",
    "    X_train_tf, y_train_tf,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_tf, y_val_tf, val_sample_weights),\n",
    "    sample_weight=train_sample_weights,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "\n",
    "# Training predictions\n",
    "y_train_prob_nn = nn_model.predict(X_train_tf, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_train_pred_nn = (y_train_prob_nn > 0.5).astype(int)\n",
    "\n",
    "# Validation predictions  \n",
    "y_val_prob_nn = nn_model.predict(X_val_tf, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_val_pred_nn = (y_val_prob_nn > 0.5).astype(int)\n",
    "\n",
    "# Test predictions\n",
    "y_test_prob_nn = nn_model.predict(X_test_tf, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_test_pred_nn = (y_test_prob_nn > 0.5).astype(int)\n",
    "\n",
    "# Evaluations\n",
    "nn_train_metrics = evaluate_model_performance(y_train_tf.astype(int), y_train_pred_nn, y_train_prob_nn, \"Neural Network Training\")\n",
    "nn_val_metrics = evaluate_model_performance(y_val_tf.astype(int), y_val_pred_nn, y_val_prob_nn, \"Neural Network Validation\") \n",
    "nn_test_metrics = evaluate_model_performance(y_test_tf.astype(int), y_test_pred_nn, y_test_prob_nn, \"Neural Network Test\")\n",
    "\n",
    "# Top K Risk Analysis for Neural Network\n",
    "analyze_top_k_predictions(y_val_tf.astype(int), y_val_prob_nn)\n",
    "\n",
    "# Training history summary\n",
    "print(\"Training History Summary:\")\n",
    "print(f\"- Total epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"- Best epoch (lowest val_loss): {np.argmin(history.history['val_loss']) + 1}\")\n",
    "print(f\"- Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"- Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"- Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if 'val_auc' in history.history:\n",
    "    print(f\"- Final validation AUC: {history.history['val_auc'][-1]:.4f}\")\n",
    "    print(f\"- Best validation AUC: {max(history.history['val_auc']):.4f}\")\n",
    "\n",
    "# Model complexity summary\n",
    "total_params = nn_model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in nn_model.trainable_weights])\n",
    "\n",
    "print(f\"\\nModel Complexity:\")\n",
    "print(f\"- Total parameters: {total_params:,}\")\n",
    "print(f\"- Trainable parameters: {trainable_params:,}\") \n",
    "print(f\"- Model size: ~{total_params * 4 / 1024:.1f} KB (float32)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
