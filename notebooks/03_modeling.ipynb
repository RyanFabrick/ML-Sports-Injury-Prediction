{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefcd3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanf\\Desktop\\ML Sports Prediction\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "import shap\n",
    "from datetime import datetime\n",
    "\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import time\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552320cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization settings\\\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958e3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and etc:\n",
      "- Target: injury_next_14_days\n",
      "- Random State: 42\n",
      "- Model Name: nba_injury_predictor_v1\n",
      "- TensorFlow version: 2.19.0\n",
      "- GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration and random seeds\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COLUMN = 'injury_next_14_days'\n",
    "MODEL_NAME = 'nba_injury_predictor_v1'\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Configuration and etc:\")\n",
    "print(f\"- Target: {TARGET_COLUMN}\")\n",
    "print(f\"- Random State: {RANDOM_STATE}\")\n",
    "print(f\"- Model Name: {MODEL_NAME}\")\n",
    "print(f\"- TensorFlow version: {tf.__version__}\")\n",
    "print(f\"- GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29ab9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully:\n",
      "- Training: (6975, 34) features, 6975 samples\n",
      "- Validation: (2567, 34) features, 2567 samples\n",
      "- Test: (589, 34) features, 589 samples\n",
      "- Selected features: 34\n",
      "- Class weights: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "- Preprocessing config loaded\n",
      "- Feature selection metadata loaded\n",
      "- Data split validation loaded\n",
      "- Feature consistency across all splits\n",
      "\n",
      "Target distribution validation:\n",
      "- Training positive rate: 2.4% (after SMOTE)\n",
      "- Validation positive rate: 3.0%\n",
      "- Test positive rate: 1.0%\n",
      "- No missing values in any split\n",
      "- All features are numeric\n",
      "\n",
      "All data validation checks passed!\n",
      "\n",
      "Feature Statistics (Training Data):\n",
      "- Mean range: -0.000 to 1117.278\n",
      "- Std range: 0.029 to 280.307\n",
      "- Min values: -11.500 to 23.750\n",
      "- Max values: 0.170 to 1757.000\n",
      "- Features with std > 10: 5 (may need scaling)\n",
      "\n",
      "Class Balance Check:\n",
      "- Training: {0: 6808, 1: 167}\n",
      "- Validation: {0: 2490, 1: 77}\n",
      "- Test: {0: 583, 1: 6}\n",
      "\n",
      "Sample Features by Type:\n",
      "- Workload features (7): ['total_actions_30d', 'shooting_load_30d', 'defensive_load_30d']...\n",
      "- Fatigue features (3): ['rest_days_since_last', 'is_back_to_back', 'fatigue_score']...\n",
      "- Context features (3): ['contact_usage_rate', 'bmi', 'age_at_game']...\n",
      "- Features scaled using RobustScaler\n",
      "  - Training scaled shape: (6975, 34)\n",
      "  - Scaled feature stats: mean≈0.147, std≈1.505\n",
      "- Data converted to TensorFlow format\n",
      "  - Input shape: (6975, 34)\n",
      "  - Target shape: (6975,)\n",
      "  - Data types: float32, float32\n",
      "- Scaler saved for deployment\n",
      "Data loaded and prepared for modeling stage\n",
      "Prepared to build TensorFlow model with 34 features\n"
     ]
    }
   ],
   "source": [
    "# Loads all processed data\n",
    "# Training data (SMOTE balanced + feature selected)\n",
    "X_train = pd.read_csv('../data/processed/X_train_final.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train_final.csv').squeeze()\n",
    "\n",
    "# Validation data (feature selected)\n",
    "X_val = pd.read_csv('../data/processed/X_validation_final.csv')\n",
    "y_val = pd.read_csv('../data/processed/y_validation_final.csv').squeeze()\n",
    "\n",
    "# Test data (feature selected)\n",
    "X_test = pd.read_csv('../data/processed/X_test_final.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test_final.csv').squeeze()\n",
    "\n",
    "print(f\"Data loaded successfully:\")\n",
    "print(f\"- Training: {X_train.shape} features, {len(y_train)} samples\")\n",
    "print(f\"- Validation: {X_val.shape} features, {len(y_val)} samples\") \n",
    "print(f\"- Test: {X_test.shape} features, {len(y_test)} samples\")\n",
    "\n",
    "# Loads metadata and configuration\n",
    "# Selected features list\n",
    "selected_features = joblib.load('../data/processed/selected_features.pkl')\n",
    "print(f\"- Selected features: {len(selected_features)}\")\n",
    "\n",
    "# Class weights for handling imbalance\n",
    "class_weights = joblib.load('../data/processed/class_weights.pkl')\n",
    "print(f\"- Class weights: {class_weights}\")\n",
    "\n",
    "# Preprocessing configuration\n",
    "preprocessing_config = joblib.load('../data/processed/preprocessing_config.pkl')\n",
    "print(f\"- Preprocessing config loaded\")\n",
    "\n",
    "# Feature selection results\n",
    "feature_selection_results = joblib.load('../data/processed/feature_selection_results.pkl')\n",
    "print(f\"- Feature selection metadata loaded\")\n",
    "\n",
    "# Split information for validation\n",
    "split_info = joblib.load('../data/processed/split_info.pkl')\n",
    "print(f\"- Data split validation loaded\")\n",
    "\n",
    "# Data validation and consistency checks\n",
    "# Check feature consistency\n",
    "assert list(X_train.columns) == selected_features, \"Training features don't match selected features\"\n",
    "assert list(X_val.columns) == selected_features, \"Validation features don't match selected features\"  \n",
    "assert list(X_test.columns) == selected_features, \"Test features don't match selected features\"\n",
    "print(\"- Feature consistency across all splits\")\n",
    "\n",
    "# Checks target distributions\n",
    "train_positive_rate = y_train.mean()\n",
    "val_positive_rate = y_val.mean()\n",
    "test_positive_rate = y_test.mean()\n",
    "\n",
    "print(f\"\\nTarget distribution validation:\")\n",
    "print(f\"- Training positive rate: {train_positive_rate:.1%} (after SMOTE)\")\n",
    "print(f\"- Validation positive rate: {val_positive_rate:.1%}\")\n",
    "print(f\"- Test positive rate: {test_positive_rate:.1%}\")\n",
    "\n",
    "# Checks for missing values\n",
    "train_missing = X_train.isnull().sum().sum()\n",
    "val_missing = X_val.isnull().sum().sum()\n",
    "test_missing = X_test.isnull().sum().sum()\n",
    "\n",
    "assert train_missing == 0, f\"Training data has {train_missing} missing values\"\n",
    "assert val_missing == 0, f\"Validation data has {val_missing} missing values\"\n",
    "assert test_missing == 0, f\"Test data has {test_missing} missing values\"\n",
    "print(\"- No missing values in any split\")\n",
    "\n",
    "# Verifies data types\n",
    "assert X_train.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in training\"\n",
    "assert X_val.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in validation\"\n",
    "assert X_test.dtypes.apply(lambda x: x.kind in 'biufc').all(), \"Non-numeric features in test\"\n",
    "print(\"- All features are numeric\")\n",
    "\n",
    "print(\"\\nAll data validation checks passed!\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nFeature Statistics (Training Data):\")\n",
    "print(f\"- Mean range: {X_train.mean().min():.3f} to {X_train.mean().max():.3f}\")\n",
    "print(f\"- Std range: {X_train.std().min():.3f} to {X_train.std().max():.3f}\")\n",
    "print(f\"- Min values: {X_train.min().min():.3f} to {X_train.min().max():.3f}\")\n",
    "print(f\"- Max values: {X_train.max().min():.3f} to {X_train.max().max():.3f}\")\n",
    "\n",
    "# Checks for potential scaling issues\n",
    "features_need_scaling = (X_train.std() > 10).sum()\n",
    "print(f\"- Features with std > 10: {features_need_scaling} (may need scaling)\")\n",
    "\n",
    "# Targets class balance verification\n",
    "print(f\"\\nClass Balance Check:\")\n",
    "print(f\"- Training: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"- Validation: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"- Test: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Sample feature names by category\n",
    "print(f\"\\nSample Features by Type:\")\n",
    "workload_features = [f for f in selected_features if any(x in f for x in ['_7d', '_30d', 'load'])]\n",
    "fatigue_features = [f for f in selected_features if any(x in f for x in ['fatigue', 'rest', 'back_to_back'])]\n",
    "context_features = [f for f in selected_features if any(x in f for x in ['age', 'bmi', 'position'])]\n",
    "\n",
    "print(f\"- Workload features ({len(workload_features)}): {workload_features[:3]}...\")\n",
    "print(f\"- Fatigue features ({len(fatigue_features)}): {fatigue_features[:3]}...\")\n",
    "print(f\"- Context features ({len(context_features)}): {context_features[:3]}...\")\n",
    "\n",
    "\n",
    "# Data preprocessing for modeling\n",
    "# Feature scaling\n",
    "# RobustScaler to handle outliers better than StandardScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=selected_features, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=selected_features, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=selected_features, index=X_test.index)\n",
    "\n",
    "print(f\"- Features scaled using RobustScaler\")\n",
    "print(f\"  - Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  - Scaled feature stats: mean≈{X_train_scaled.mean().mean():.3f}, std≈{X_train_scaled.std().mean():.3f}\")\n",
    "\n",
    "# Converts to numpy arrays for TensorFlow\n",
    "X_train_tf = X_train_scaled.values.astype(np.float32)\n",
    "X_val_tf = X_val_scaled.values.astype(np.float32)\n",
    "X_test_tf = X_test_scaled.values.astype(np.float32)\n",
    "y_train_tf = y_train.values.astype(np.float32)\n",
    "y_val_tf = y_val.values.astype(np.float32)\n",
    "y_test_tf = y_test.values.astype(np.float32)\n",
    "\n",
    "print(f\"- Data converted to TensorFlow format\")\n",
    "print(f\"  - Input shape: {X_train_tf.shape}\")\n",
    "print(f\"  - Target shape: {y_train_tf.shape}\")\n",
    "print(f\"  - Data types: {X_train_tf.dtype}, {y_train_tf.dtype}\")\n",
    "\n",
    "# Stores scaler for later use\n",
    "joblib.dump(scaler, f'../data/processed/{MODEL_NAME}_scaler.pkl')\n",
    "print(f\"- Scaler saved for deployment\")\n",
    "\n",
    "print(\"Data loaded and prepared for modeling stage\")\n",
    "print(f\"Prepared to build TensorFlow model with {X_train_tf.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05676021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes after all preprocessing:\n",
      "  - X_train: (6975, 34)\n",
      "  - y_train: (6975,)\n",
      "  - X_val: (2567, 34)\n",
      "  - y_val: (2567,)\n",
      "  - X_test: (589, 34)\n",
      "  - y_test: (589,)\n",
      "\n",
      "Class distribution summary:\n",
      "  - Training: [6808  167] (ratio: 40.8:1)\n",
      "  - Validation: [2490   77] (ratio: 32.3:1)\n",
      "  - Test: [583   6] (ratio: 97.2:1)\n",
      "\n",
      "Class weights for model: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "\n",
      "Ready\n",
      "   - Features: 34\n",
      "   - Training samples: 6,975\n",
      "   - Target: injury_next_14_days\n",
      "   - Model: nba_injury_predictor_v1\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "print(\"Data shapes after all preprocessing:\")\n",
    "print(f\"  - X_train: {X_train_tf.shape}\")\n",
    "print(f\"  - y_train: {y_train_tf.shape}\")\n",
    "print(f\"  - X_val: {X_val_tf.shape}\")\n",
    "print(f\"  - y_val: {y_val_tf.shape}\")\n",
    "print(f\"  - X_test: {X_test_tf.shape}\")\n",
    "print(f\"  - y_test: {y_test_tf.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution summary:\")\n",
    "print(f\"  - Training: {np.bincount(y_train_tf.astype(int))} (ratio: {(y_train_tf == 0).sum()/(y_train_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Validation: {np.bincount(y_val_tf.astype(int))} (ratio: {(y_val_tf == 0).sum()/(y_val_tf == 1).sum():.1f}:1)\")\n",
    "print(f\"  - Test: {np.bincount(y_test_tf.astype(int))} (ratio: {(y_test_tf == 0).sum()/(y_test_tf == 1).sum():.1f}:1)\")\n",
    "\n",
    "print(f\"\\nClass weights for model: {class_weights}\")\n",
    "\n",
    "print(f\"\\nReady\")\n",
    "print(f\"   - Features: {X_train_tf.shape[1]}\")\n",
    "print(f\"   - Training samples: {X_train_tf.shape[0]:,}\")\n",
    "print(f\"   - Target: {TARGET_COLUMN}\")\n",
    "print(f\"   - Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4232ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training scaled shape: (6975, 34)\n",
      "- Feature stats after scaling: mean=0.147, std=4.644\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"- Training scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"- Feature stats after scaling: mean={X_train_scaled.mean():.3f}, std={X_train_scaled.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c81266",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "### Overview\n",
    "\n",
    "The model shows strong performance on the training set but experiences degradation on validation and test sets, suggesting potential overfitting. The low precision scores across all sets indicate challenges with the severe class imbalance in injury prediction. The feature importance reveals interesting patterns where some high activity metrics actually appear protective, possibly due to conditioning effects or survivor bias in the data.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "- Random state for reproducibility\n",
    "- Class weights to handle class imbalance\n",
    "- L2 regularization with penalty strength of 1.0\n",
    "- Maximum 1000 iterations to ensure convergence\n",
    "- LBFGS solver optimized for small datasets\n",
    "\n",
    "### Performance\n",
    "\n",
    "**Training Set Performance**\n",
    "- Accuracy: 74.7%\n",
    "- Precision: 6.7%\n",
    "- Recall: 74.3%\n",
    "- F1-Score: 12.3%\n",
    "- ROC AUC: 83.0%\n",
    "- PR AUC: 18.5%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 5,088\n",
    "- False Positives: 1,720\n",
    "- False Negatives: 43\n",
    "- True Positives: 124\n",
    "\n",
    "**Validation Set Performance**\n",
    "- Accuracy: 75.7%\n",
    "- Precision: 5.7%\n",
    "- Recall: 45.5%\n",
    "- F1-Score: 10.1%\n",
    "- ROC AUC: 64.5%\n",
    "- PR AUC: 9.8%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 1,907\n",
    "- False Positives: 583\n",
    "- False Negatives: 42\n",
    "- True Positives: 35\n",
    "\n",
    "**Test Set Performance**\n",
    "- Accuracy: 47.7%\n",
    "- Precision: 0.7%\n",
    "- Recall: 33.3%\n",
    "- F1-Score: 1.3%\n",
    "- ROC AUC: 37.2%\n",
    "- PR AUC: 0.9%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 279\n",
    "- False Positives: 304\n",
    "- False Negatives: 4\n",
    "- True Positives: 2\n",
    "\n",
    "### Top K Risk Prediction Analysis\n",
    "\n",
    "- **Top 5%**: Captures 14.3% of all injuries with 8.6% precision\n",
    "- **Top 10%**: Captures 27.3% of all injuries with 8.2% precision\n",
    "- **Top 15%**: Captures 32.5% of all injuries with 6.5% precision\n",
    "- **Top 20%**: Captures 37.7% of all injuries with 5.7% precision\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "**Features that Increase Injury Risk:**\n",
    "1. **Fatigue Score** (coefficient: 1.818) - Strong positive predictor\n",
    "2. **Total Actions** (coefficient: 1.417) - Higher activity increases risk\n",
    "3. **Shooting Load 30d** (coefficient: 1.175) - Cumulative shooting strain\n",
    "4. **Is Low Performance** (coefficient: 1.132) - Poor performance indicator\n",
    "5. **Defensive Load 30d** (coefficient: 0.905) - Defensive workload impact\n",
    "6. **Total Actions 30d** (coefficient: 0.437) - Monthly activity accumulation\n",
    "\n",
    "**Features that Decrease Injury Risk:**\n",
    "1. **Is Back to Back** (coefficient: -1.844) - Counterintuitive protective factor\n",
    "2. **Cumulative Actions 30d** (coefficient: -1.467) - Higher cumulative load protective\n",
    "3. **Rebounds** (coefficient: -0.978) - Rebounding activity protective\n",
    "4. **Missed Shots** (coefficient: -0.951) - More missed shots lower risk\n",
    "5. **Total Shot Attempts** (coefficient: -0.608) - Higher shooting volume protective\n",
    "6. **Game Day of Week** (coefficient: -0.502) - Certain days lower risk\n",
    "7. **Shooting Efficiency** (coefficient: -0.403) - Better efficiency protective\n",
    "8. **Contact Usage Rate** (coefficient: -0.376) - Higher contact usage protective\n",
    "9. **Is Weekend Game** (coefficient: -0.369) - Weekend games lower risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41eb2c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING Performance:\n",
      "- Accuracy: 0.747\n",
      "- Precision: 0.067\n",
      "- Recall: 0.743\n",
      "- F1-Score: 0.123\n",
      "- ROC AUC: 0.830\n",
      "- PR AUC: 0.185\n",
      "Confusion Matrix:\n",
      "    TN: 5088 | FP: 1720\n",
      "    FN:   43 | TP:  124\n",
      "\n",
      "VALIDATION Performance:\n",
      "- Accuracy: 0.757\n",
      "- Precision: 0.057\n",
      "- Recall: 0.455\n",
      "- F1-Score: 0.101\n",
      "- ROC AUC: 0.645\n",
      "- PR AUC: 0.098\n",
      "Confusion Matrix:\n",
      "    TN: 1907 | FP:  583\n",
      "    FN:   42 | TP:   35\n",
      "\n",
      "TEST Performance:\n",
      "- Accuracy: 0.477\n",
      "- Precision: 0.007\n",
      "- Recall: 0.333\n",
      "- F1-Score: 0.013\n",
      "- ROC AUC: 0.372\n",
      "- PR AUC: 0.009\n",
      "Confusion Matrix:\n",
      "    TN:  279 | FP:  304\n",
      "    FN:    4 | TP:    2\n"
     ]
    }
   ],
   "source": [
    "# Logisitc Regression\n",
    "logistic_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight=class_weights,  # Handle class imbalance\n",
    "    penalty='l2',                # L2 regularization\n",
    "    C=1.0,                      # Regularization strength (will tune later)\n",
    "    max_iter=1000,              # Ensure convergence\n",
    "    solver='lbfgs'              # Good for small datasets\n",
    ")\n",
    "\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "# Predictions on all sets\n",
    "y_train_pred = logistic_model.predict(X_train_scaled)\n",
    "y_val_pred = logistic_model.predict(X_val_scaled)\n",
    "y_test_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_prob = logistic_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_prob = logistic_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_prob = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "def evaluate_model_performance(y_true, y_pred, y_prob, dataset_name):\n",
    "    \"\"\"\n",
    "    Model evaluation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{dataset_name.upper()} Performance:\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # AUC metrics\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    print(f\"- Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"- Precision: {precision:.3f}\")\n",
    "    print(f\"- Recall: {recall:.3f}\")\n",
    "    print(f\"- F1-Score: {f1:.3f}\")\n",
    "    print(f\"- ROC AUC: {roc_auc:.3f}\")\n",
    "    print(f\"- PR AUC: {pr_auc:.3f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(f\"    TN: {cm[0,0]:4d} | FP: {cm[0,1]:4d}\")\n",
    "    print(f\"    FN: {cm[1,0]:4d} | TP: {cm[1,1]:4d}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc, 'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluates on all datasets\n",
    "train_metrics = evaluate_model_performance(y_train, y_train_pred, y_train_prob, \"Training\")\n",
    "val_metrics = evaluate_model_performance(y_val, y_val_pred, y_val_prob, \"Validation\")\n",
    "test_metrics = evaluate_model_performance(y_test, y_test_pred, y_test_prob, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e876b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  14.3% of injuries captured, precision =   8.6%\n",
      "    Top 10%:  27.3% of injuries captured, precision =   8.2%\n",
      "    Top 15%:  32.5% of injuries captured, precision =   6.5%\n",
      "    Top 20%:  37.7% of injuries captured, precision =   5.7%\n"
     ]
    }
   ],
   "source": [
    "# Top K Risk Prediciton Analysis\n",
    "\n",
    "def analyze_top_k_predictions(y_true, y_prob, k_values=[5, 10, 15, 20]):\n",
    "    \"\"\"\n",
    "    Analyzes what percentage of actual injuries are captured in top K% predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Top K Risk Prediction Performance:\")\n",
    "    \n",
    "    # Sorts by probability (highest risk first)\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    sorted_true = y_true[sorted_indices]\n",
    "    \n",
    "    total_positives = y_true.sum()\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Top k% of predictions\n",
    "        top_k_size = int(n_samples * k / 100)\n",
    "        top_k_true = sorted_true[:top_k_size]\n",
    "        \n",
    "        # Calculates capture rate\n",
    "        captured_positives = top_k_true.sum()\n",
    "        capture_rate = captured_positives / total_positives if total_positives > 0 else 0\n",
    "        precision_at_k = captured_positives / top_k_size if top_k_size > 0 else 0\n",
    "        \n",
    "        print(f\"    Top {k:2d}%: {capture_rate*100:5.1f}% of injuries captured, \"\n",
    "              f\"precision = {precision_at_k*100:5.1f}%\")\n",
    "\n",
    "# Analyzes on validation set\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50fafd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Most Important Features:\n",
      "Feature                   Coefficient  Impact         \n",
      "  is_back_to_back               -1.844   Decrease Injury Risk\n",
      "  fatigue_score                  1.818   Increase Injury Risk\n",
      "  cumulative_actions_30d        -1.467   Decrease Injury Risk\n",
      "  total_actions                  1.417   Increase Injury Risk\n",
      "  shooting_load_30d              1.175   Increase Injury Risk\n",
      "  is_low_performance             1.132   Increase Injury Risk\n",
      "  rebounds                      -0.978   Decrease Injury Risk\n",
      "  missed_shots                  -0.951   Decrease Injury Risk\n",
      "  defensive_load_30d             0.905   Increase Injury Risk\n",
      "  total_shot_attempts           -0.608   Decrease Injury Risk\n",
      "  game_day_of_week              -0.502   Decrease Injury Risk\n",
      "  total_actions_30d              0.437   Increase Injury Risk\n",
      "  shooting_efficiency           -0.403   Decrease Injury Risk\n",
      "  contact_usage_rate            -0.376   Decrease Injury Risk\n",
      "  is_weekend_game               -0.369   Decrease Injury Risk\n"
     ]
    }
   ],
   "source": [
    "# Feature importance Analysis\n",
    "\n",
    "# Gets feature coefficients (weights)\n",
    "feature_coefficients = logistic_model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'coefficient': feature_coefficients,\n",
    "    'abs_coefficient': np.abs(feature_coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"Top 15 Most Important Features:\")\n",
    "print(f\"{'Feature':<25} {'Coefficient':<12} {'Impact':<15}\")\n",
    "\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    impact = \"Increase Injury Risk\" if row['coefficient'] > 0 else \"Decrease Injury Risk\"\n",
    "    print(f\"  {row['feature']:<25} {row['coefficient']:>10.3f}   {impact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5c346",
   "metadata": {},
   "source": [
    "# Random Forest Model\n",
    "\n",
    "### Overview\n",
    "\n",
    "The Random Forest model demonstrates classic signs of overfitting with near perfect training performance but degradation on validation and test sets. Despite hyperparameter tuning with 162 parameter combinations, the model struggles w/ generalization. The feature importance rankings reveal fatigue score as the dominant predictor, with game timing and performance trend features playing secondary roles.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "**Hyperparameter Grid Search:**\n",
    "- 162 parameter combinations tested across 3 fold cross validation (486 total fits)\n",
    "- Training time: 76.0 seconds\n",
    "- Scoring metric: Average Precision (PR-AUC) for imbalanced data\n",
    "- Stratified K-Fold cross-validation with 3 splits\n",
    "\n",
    "**Best Parameters:**\n",
    "- Number of estimators: 150\n",
    "- Maximum depth: 15\n",
    "- Minimum samples split: 50\n",
    "- Minimum samples leaf: 20\n",
    "- Maximum features: sqrt\n",
    "- Class weight: balanced_subsample\n",
    "- Best cross-validation PR-AUC: 0.1729\n",
    "\n",
    "### Performance\n",
    "\n",
    "**Training Set Performance**\n",
    "- Accuracy: 98.5%\n",
    "- Precision: 62.4%\n",
    "- Recall: 96.4%\n",
    "- F1-Score: 75.8%\n",
    "- ROC AUC: 99.6%\n",
    "- PR AUC: 81.2%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 6,711\n",
    "- False Positives: 97\n",
    "- False Negatives: 6\n",
    "- True Positives: 161\n",
    "\n",
    "**Validation Set Performance**\n",
    "- Accuracy: 96.1%\n",
    "- Precision: 23.9%\n",
    "- Recall: 14.3%\n",
    "- F1-Score: 17.9%\n",
    "- ROC AUC: 64.7%\n",
    "- PR AUC: 12.1%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 2,455\n",
    "- False Positives: 35\n",
    "- False Negatives: 66\n",
    "- True Positives: 11\n",
    "\n",
    "**Test Set Performance**\n",
    "- Accuracy: 94.7%\n",
    "- Precision: 3.7%\n",
    "- Recall: 16.7%\n",
    "- F1-Score: 6.1%\n",
    "- ROC AUC: 35.6%\n",
    "- PR AUC: 1.5%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 557\n",
    "- False Positives: 26\n",
    "- False Negatives: 5\n",
    "- True Positives: 1\n",
    "\n",
    "### Top K Risk Prediction Analysis\n",
    "\n",
    "- **Top 5%**: Captures 20.8% of all injuries with 12.5% precision\n",
    "- **Top 10%**: Captures 26.0% of all injuries with 7.8% precision\n",
    "- **Top 15%**: Captures 33.8% of all injuries with 6.8% precision\n",
    "- **Top 20%**: Captures 40.3% of all injuries with 6.0% precision\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "**Top 15 Most Important Features:**\n",
    "\n",
    "1. **Fatigue Score** (0.1538) - Dominant predictor representing player exhaustion\n",
    "2. **Game Day of Week** (0.0658) - Scheduling patterns impact injury risk\n",
    "3. **Current vs 14-Day Average** (0.0487) - Recent performance relative to baseline\n",
    "4. **Performance Drop 7 vs 30** (0.0482) - Short-term performance decline indicator\n",
    "5. **Total Actions** (0.0457) - Overall game activity level\n",
    "6. **Shots vs Season Average** (0.0387) - Shooting volume relative to season norm\n",
    "7. **Efficiency Trend 7d** (0.0358) - Recent efficiency trajectory\n",
    "8. **Contact Usage Rate** (0.0349) - Physical contact involvement\n",
    "9. **Actions Trend 7d** (0.0331) - Recent activity pattern changes\n",
    "10. **Rebounds vs Season Average** (0.0325) - Rebounding relative to baseline\n",
    "11. **Shooting Efficiency Decline** (0.0304) - Deterioration in shooting performance\n",
    "12. **Substitution Frequency** (0.0300) - In-game replacement patterns\n",
    "13. **Shooting Load 30d** (0.0282) - Cumulative shooting workload\n",
    "14. **Defensive Load 30d** (0.0277) - Defensive activity accumulation\n",
    "15. **Cumulative Actions 30d** (0.0277) - Monthly activity total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d968772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Training completed in 117.7 seconds\n",
      "Best parameters: {'class_weight': 'balanced_subsample', 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 20, 'min_samples_split': 50, 'n_estimators': 150, 'random_state': 42}\n",
      "Best cross-validation PR-AUC: 0.1729\n",
      "\n",
      "TRAINING Performance:\n",
      "- Accuracy: 0.985\n",
      "- Precision: 0.624\n",
      "- Recall: 0.964\n",
      "- F1-Score: 0.758\n",
      "- ROC AUC: 0.996\n",
      "- PR AUC: 0.812\n",
      "Confusion Matrix:\n",
      "    TN: 6711 | FP:   97\n",
      "    FN:    6 | TP:  161\n",
      "\n",
      "VALIDATION Performance:\n",
      "- Accuracy: 0.961\n",
      "- Precision: 0.239\n",
      "- Recall: 0.143\n",
      "- F1-Score: 0.179\n",
      "- ROC AUC: 0.647\n",
      "- PR AUC: 0.121\n",
      "Confusion Matrix:\n",
      "    TN: 2455 | FP:   35\n",
      "    FN:   66 | TP:   11\n",
      "\n",
      "TEST Performance:\n",
      "- Accuracy: 0.947\n",
      "- Precision: 0.037\n",
      "- Recall: 0.167\n",
      "- F1-Score: 0.061\n",
      "- ROC AUC: 0.356\n",
      "- PR AUC: 0.015\n",
      "Confusion Matrix:\n",
      "    TN:  557 | FP:   26\n",
      "    FN:    5 | TP:    1\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  20.8% of injuries captured, precision =  12.5%\n",
      "    Top 10%:  26.0% of injuries captured, precision =   7.8%\n",
      "    Top 15%:  33.8% of injuries captured, precision =   6.8%\n",
      "    Top 20%:  40.3% of injuries captured, precision =   6.0%\n",
      "Top 15 Most Important Features (Random Forest):\n",
      "Feature                   Importance  \n",
      "fatigue_score                 0.1538\n",
      "game_day_of_week              0.0658\n",
      "current_vs_14day_avg          0.0487\n",
      "performance_drop_7vs30        0.0482\n",
      "total_actions                 0.0457\n",
      "shots_vs_season_avg           0.0387\n",
      "efficiency_trend_7d           0.0358\n",
      "contact_usage_rate            0.0349\n",
      "actions_trend_7d              0.0331\n",
      "rebounds_vs_season_avg        0.0325\n",
      "shooting_eff_decline          0.0304\n",
      "substitution_frequency        0.0300\n",
      "shooting_load_30d             0.0282\n",
      "defensive_load_30d            0.0277\n",
      "cumulative_actions_30d        0.0277\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "\n",
    "# Hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 150], \n",
    "    'max_depth': [5, 10, 15],      \n",
    "    'min_samples_split': [20, 50, 100], \n",
    "    'min_samples_leaf': [5, 10, 20],    \n",
    "    'max_features': ['sqrt', 0.5],      \n",
    "    'class_weight': ['balanced_subsample'],\n",
    "    'random_state': [RANDOM_STATE]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Grid Search with Cross Validation\n",
    "cv_folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=cv_folds,\n",
    "    scoring='average_precision',  # PR-AUC for imbalanced data\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time:.1f} seconds\")\n",
    "print(f\"Best parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation PR-AUC: {rf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on all sets\n",
    "y_train_pred_rf = rf_model.predict(X_train_scaled)\n",
    "y_val_pred_rf = rf_model.predict(X_val_scaled)\n",
    "y_test_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Prediction probabilities\n",
    "y_train_prob_rf = rf_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_prob_rf = rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluations\n",
    "train_metrics_rf = evaluate_model_performance(y_train, y_train_pred_rf, y_train_prob_rf, \"Training\")\n",
    "val_metrics_rf = evaluate_model_performance(y_val, y_val_pred_rf, y_val_prob_rf, \"Validation\")\n",
    "test_metrics_rf = evaluate_model_performance(y_test, y_test_pred_rf, y_test_prob_rf, \"Test\")\n",
    "\n",
    "# Top K Risk Analysis\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob_rf)\n",
    "\n",
    "# Feature Importance Analysis\n",
    "\n",
    "# Gets feature importances\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features (Random Forest):\")\n",
    "print(f\"{'Feature':<25} {'Importance':<12}\")\n",
    "\n",
    "for idx, row in feature_importance_rf.head(15).iterrows():\n",
    "    print(f\"{row['feature']:<25} {row['importance']:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865e5a4",
   "metadata": {},
   "source": [
    "# XGBoost Model\n",
    "\n",
    "### Overview\n",
    "\n",
    "The XGBoost model demonstrates more balanced performance compared to the Random Forest, avoiding extreme overfitting while maintaining reasonable generalization. The hyperparameter search across 2,187 parameter combinations resulted in a model that shows moderate performance degradation from training to test sets. The feature importance distribution is more balanced than Random Forest, w/ fatigue score remaining the top predictor but at a lower dominance level.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "**Class Distribution Handling:**\n",
    "- Negative class samples: 6,808\n",
    "- Positive class samples: 167\n",
    "- Scale positive weight: 40.77 (automatic class imbalance adjustment)\n",
    "\n",
    "**Hyperparameter Grid Search:**\n",
    "- 2,187 parameter combinations tested across 3-fold cross-validation (6,561 total fits)\n",
    "- Scoring metric: Average Precision (PR-AUC) for imbalanced data\n",
    "- Stratified K-Fold cross-validation with 3 splits\n",
    "- Early stopping with 10 rounds patience\n",
    "\n",
    "**Best Parameters:**\n",
    "- Maximum depth: 3\n",
    "- Learning rate: 0.1\n",
    "- Number of estimators: 100\n",
    "- Subsample ratio: 0.7\n",
    "- Column subsample ratio: 0.7\n",
    "- L1 regularization (reg_alpha): 0.5\n",
    "- L2 regularization (reg_lambda): 2\n",
    "- Best cross-validation PR-AUC: 0.143\n",
    "\n",
    "### Performance\n",
    "\n",
    "**Training Set Performance**\n",
    "- Accuracy: 82.2%\n",
    "- Precision: 9.7%\n",
    "- Recall: 77.2%\n",
    "- F1-Score: 17.2%\n",
    "- ROC AUC: 88.4%\n",
    "- PR AUC: 26.1%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 5,607\n",
    "- False Positives: 1,201\n",
    "- False Negatives: 38\n",
    "- True Positives: 129\n",
    "\n",
    "**Validation Set Performance**\n",
    "- Accuracy: 85.1%\n",
    "- Precision: 7.5%\n",
    "- Recall: 35.1%\n",
    "- F1-Score: 12.4%\n",
    "- ROC AUC: 69.0%\n",
    "- PR AUC: 8.3%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 2,158\n",
    "- False Positives: 332\n",
    "- False Negatives: 50\n",
    "- True Positives: 27\n",
    "\n",
    "**Test Set Performance**\n",
    "- Accuracy: 72.7%\n",
    "- Precision: 0.6%\n",
    "- Recall: 16.7%\n",
    "- F1-Score: 1.2%\n",
    "- ROC AUC: 39.6%\n",
    "- PR AUC: 1.1%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 427\n",
    "- False Positives: 156\n",
    "- False Negatives: 5\n",
    "- True Positives: 1\n",
    "\n",
    "### Top K Risk Prediction Analysis\n",
    "\n",
    "- **Top 5%**: Captures 14.3% of all injuries with 8.6% precision\n",
    "- **Top 10%**: Captures 23.4% of all injuries with 7.0% precision\n",
    "- **Top 15%**: Captures 39.0% of all injuries with 7.8% precision\n",
    "- **Top 20%**: Captures 51.9% of all injuries with 7.8% precision\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "**Top 20 Most Important Features:**\n",
    "\n",
    "1. **Fatigue Score** (0.0806) - Primary exhaustion indicator\n",
    "2. **Rebounds vs Season Average** (0.0611) - Rebounding performance relative to baseline\n",
    "3. **Current vs 14-Day Average** (0.0526) - Recent performance comparison\n",
    "4. **Substitution Frequency** (0.0520) - In-game replacement patterns\n",
    "5. **Game Day of Week** (0.0480) - Weekly scheduling impact\n",
    "6. **Shots vs Season Average** (0.0434) - Shooting volume deviation\n",
    "7. **Missed Shots** (0.0426) - Shooting inefficiency metric\n",
    "8. **Is Back to Back** (0.0420) - Consecutive game indicator\n",
    "9. **Rest Days Since Last** (0.0394) - Recovery time between games\n",
    "10. **Defensive Load 30d** (0.0380) - Monthly defensive workload\n",
    "11. **Shooting Efficiency Decline** (0.0349) - Performance deterioration\n",
    "12. **Contact Usage Rate** (0.0347) - Physical involvement level\n",
    "13. **Fouls** (0.0337) - Foul accumulation indicator\n",
    "14. **Total Actions** (0.0332) - Overall activity measure\n",
    "15. **Efficiency Trend 7d** (0.0327) - Short-term efficiency pattern\n",
    "16. **Shooting Load 30d** (0.0322) - Monthly shooting workload\n",
    "17. **BMI** (0.0299) - Body mass index impact\n",
    "18. **Actions Trend 7d** (0.0275) - Recent activity changes\n",
    "19. **Performance Drop 7 vs 30** (0.0261) - Performance decline comparison\n",
    "20. **Turnovers** (0.0254) - Ball handling efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8339b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training:\n",
      "- Negative class: 6,808\n",
      "- Positive class: 167\n",
      "- Scale pos weight: 40.77\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 3 folds for each of 2187 candidates, totalling 6561 fits\n",
      "\n",
      "Best hyperparameters:\n",
      "- colsample_bytree: 0.7\n",
      "- learning_rate: 0.1\n",
      "- max_depth: 3\n",
      "- n_estimators: 100\n",
      "- reg_alpha: 0.5\n",
      "- reg_lambda: 2\n",
      "- subsample: 0.7\n",
      "Best CV PR-AUC: 0.143\n",
      "\n",
      "XGBOOST TRAINING Performance:\n",
      "- Accuracy: 0.822\n",
      "- Precision: 0.097\n",
      "- Recall: 0.772\n",
      "- F1-Score: 0.172\n",
      "- ROC AUC: 0.884\n",
      "- PR AUC: 0.261\n",
      "Confusion Matrix:\n",
      "    TN: 5607 | FP: 1201\n",
      "    FN:   38 | TP:  129\n",
      "\n",
      "XGBOOST VALIDATION Performance:\n",
      "- Accuracy: 0.851\n",
      "- Precision: 0.075\n",
      "- Recall: 0.351\n",
      "- F1-Score: 0.124\n",
      "- ROC AUC: 0.690\n",
      "- PR AUC: 0.083\n",
      "Confusion Matrix:\n",
      "    TN: 2158 | FP:  332\n",
      "    FN:   50 | TP:   27\n",
      "\n",
      "XGBOOST TEST Performance:\n",
      "- Accuracy: 0.727\n",
      "- Precision: 0.006\n",
      "- Recall: 0.167\n",
      "- F1-Score: 0.012\n",
      "- ROC AUC: 0.396\n",
      "- PR AUC: 0.011\n",
      "Confusion Matrix:\n",
      "    TN:  427 | FP:  156\n",
      "    FN:    5 | TP:    1\n",
      "Top 20 Most Important Features (XGBoost):\n",
      "Feature                        Importance  \n",
      "fatigue_score                      0.0806\n",
      "rebounds_vs_season_avg             0.0611\n",
      "current_vs_14day_avg               0.0526\n",
      "substitution_frequency             0.0520\n",
      "game_day_of_week                   0.0480\n",
      "shots_vs_season_avg                0.0434\n",
      "missed_shots                       0.0426\n",
      "is_back_to_back                    0.0420\n",
      "rest_days_since_last               0.0394\n",
      "defensive_load_30d                 0.0380\n",
      "shooting_eff_decline               0.0349\n",
      "contact_usage_rate                 0.0347\n",
      "fouls                              0.0337\n",
      "total_actions                      0.0332\n",
      "efficiency_trend_7d                0.0327\n",
      "shooting_load_30d                  0.0322\n",
      "bmi                                0.0299\n",
      "actions_trend_7d                   0.0275\n",
      "performance_drop_7vs30             0.0261\n",
      "turnovers                          0.0254\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  14.3% of injuries captured, precision =   8.6%\n",
      "    Top 10%:  23.4% of injuries captured, precision =   7.0%\n",
      "    Top 15%:  39.0% of injuries captured, precision =   7.8%\n",
      "    Top 20%:  51.9% of injuries captured, precision =   7.8%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model\n",
    "\n",
    "# XGBoost Model \n",
    "def train_xgboost_model(X_train_scaled, y_train, X_val_scaled, y_val, class_weights, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains XGBoost model\n",
    "    \"\"\"\n",
    "    # Calculates scale_pos_weight for XGBoost\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "    \n",
    "    print(f\"Class distribution in training:\")\n",
    "    print(f\"- Negative class: {neg_count:,}\")\n",
    "    print(f\"- Positive class: {pos_count:,}\")\n",
    "    print(f\"- Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Base XGBoost model \n",
    "    xgb_base = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=scale_pos_weight,  # Handles imbalance\n",
    "        random_state=random_state,\n",
    "        eval_metric=['logloss', 'auc'],\n",
    "        early_stopping_rounds=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1], \n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'reg_alpha': [0, 0.1, 0.5],  # L1 regularization\n",
    "        'reg_lambda': [1, 1.5, 2]   # L2 regularization\n",
    "    }\n",
    "    \n",
    "    # Stratified CV for imbalanced data\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Grid search with PR-AUC scoring\n",
    "    print(\"Performing hyperparameter tuning...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_base,\n",
    "        param_grid=param_grid,\n",
    "        scoring='average_precision',  # PR-AUC \n",
    "        cv=cv_strategy,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit with validation set for early stopping\n",
    "    grid_search.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"- {param}: {value}\")\n",
    "    \n",
    "    print(f\"Best CV PR-AUC: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return best_model, grid_search\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model, xgb_grid_search = train_xgboost_model(\n",
    "    X_train_scaled, y_train, \n",
    "    X_val_scaled, y_val, \n",
    "    class_weights\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "\n",
    "# Training predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_scaled)\n",
    "y_train_prob_xgb = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Validation predictions  \n",
    "y_val_pred_xgb = xgb_model.predict(X_val_scaled)\n",
    "y_val_prob_xgb = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_xgb = xgb_model.predict(X_test_scaled) \n",
    "y_test_prob_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluations\n",
    "xgb_train_metrics = evaluate_model_performance(y_train, y_train_pred_xgb, y_train_prob_xgb, \"XGBoost Training\")\n",
    "xgb_val_metrics = evaluate_model_performance(y_val, y_val_pred_xgb, y_val_prob_xgb, \"XGBoost Validation\") \n",
    "xgb_test_metrics = evaluate_model_performance(y_test, y_test_pred_xgb, y_test_prob_xgb, \"XGBoost Test\")\n",
    "\n",
    "# Feature importance analysis\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features (XGBoost):\")\n",
    "print(f\"{'Feature':<30} {'Importance':<12}\")\n",
    "for idx, row in feature_importance_xgb.head(20).iterrows():\n",
    "    print(f\"{row['feature']:<30} {row['importance']:>10.4f}\")\n",
    "\n",
    "# Top K Risk Analysis for XGBoost\n",
    "# Analyze on validation set\n",
    "analyze_top_k_predictions(y_val.values, y_val_prob_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0dfe2",
   "metadata": {},
   "source": [
    "# Neural Network Model\n",
    "\n",
    "### Overview\n",
    "\n",
    "The neural network model shows early convergence with training stopping at epoch 17 due to validation loss stagnation. Despite architecture w/ dropout layers and batch normalization, the model shows some overfitting and struggles with generalization on unseen data. The training process revealed quick initial learning followed by validation performance plateau, suggesting the model reached its capacity for this dataset relatively quickly.\n",
    "\n",
    "### Configuration\n",
    "\n",
    "**Architecture Design:**\n",
    "- Input layer: 34 features\n",
    "- Hidden layer 1: 64 units with ReLU activation, 30% dropout, batch normalization\n",
    "- Hidden layer 2: 32 units with ReLU activation, 30% dropout, batch normalization  \n",
    "- Hidden layer 3: 16 units with ReLU activation, 20% dropout, batch normalization\n",
    "- Output layer: 1 unit with sigmoid activation\n",
    "\n",
    "**Training Configuration:**\n",
    "- Training samples: 6,975\n",
    "- Class weights: {0: 0.512, 1: 20.883}\n",
    "- Batch size: 32\n",
    "- Maximum epochs: 200\n",
    "- Early stopping patience: 15 epochs\n",
    "- Learning rate reduction patience: 10 epochs\n",
    "- Initial learning rate: 0.001\n",
    "\n",
    "**Training Process:**\n",
    "- Total epochs trained: 17\n",
    "- Best epoch: 2 (lowest validation loss)\n",
    "- Training time: 11.9 seconds\n",
    "- Learning rate reduced to 0.0005 at epoch 12\n",
    "- Early stopping triggered at epoch 17\n",
    "\n",
    "### Performance\n",
    "\n",
    "**Training Set Performance**\n",
    "- Accuracy: 77.1%\n",
    "- Precision: 5.9%\n",
    "- Recall: 56.9%\n",
    "- F1-Score: 10.6%\n",
    "- ROC AUC: 73.7%\n",
    "- PR AUC: 7.7%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 5,281\n",
    "- False Positives: 1,527\n",
    "- False Negatives: 72\n",
    "- True Positives: 95\n",
    "\n",
    "**Validation Set Performance**\n",
    "- Accuracy: 74.3%\n",
    "- Precision: 5.2%\n",
    "- Recall: 44.2%\n",
    "- F1-Score: 9.3%\n",
    "- ROC AUC: 61.5%\n",
    "- PR AUC: 5.5%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 1,872\n",
    "- False Positives: 618\n",
    "- False Negatives: 43\n",
    "- True Positives: 34\n",
    "\n",
    "**Test Set Performance**\n",
    "- Accuracy: 61.6%\n",
    "- Precision: 0.9%\n",
    "- Recall: 33.3%\n",
    "- F1-Score: 1.7%\n",
    "- ROC AUC: 44.9%\n",
    "- PR AUC: 1.3%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Negatives: 361\n",
    "- False Positives: 222\n",
    "- False Negatives: 4\n",
    "- True Positives: 2\n",
    "\n",
    "### Top K Risk Prediction Analysis\n",
    "\n",
    "- **Top 5%**: Captures 11.7% of all injuries with 7.0% precision\n",
    "- **Top 10%**: Captures 20.8% of all injuries with 6.2% precision\n",
    "- **Top 15%**: Captures 26.0% of all injuries with 5.2% precision\n",
    "- **Top 20%**: Captures 36.4% of all injuries with 5.5% precision\n",
    "\n",
    "### Training History Analysis\n",
    "\n",
    "**Loss Progression:**\n",
    "- Final training loss: 0.5259\n",
    "- Final validation loss: 0.8868\n",
    "- Best validation loss: 0.7746 (achieved at epoch 2)\n",
    "- Training loss decreased consistently while validation loss increased, indicating overfitting\n",
    "\n",
    "**Model Complexity:**\n",
    "- Total parameters: 5,313\n",
    "- Trainable parameters: 5,089\n",
    "- Non-trainable parameters: 224 (batch normalization)\n",
    "- Model size: approximately 20.8 KB\n",
    "\n",
    "**Learning Dynamics:**\n",
    "- Initial rapid improvement in first 2 epochs\n",
    "- Validation performance plateau despite continued training improvements\n",
    "- Learning rate reduction at epoch 12 failed to improve validation performance\n",
    "- Early stopping prevented further overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12b2a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neural Network training...\n",
      "Input features: 34\n",
      "Training samples: 6,975\n",
      "Class weights: {0: 0.512264982373678, 1: 20.88323353293413}\n",
      "Sample weights created. Shape: (6975,)\n",
      "Sample weight distribution: (array([ 0.51226498, 20.88323353]), array([6808,  167], dtype=int64))\n",
      "\n",
      "Neural Network Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_1                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_2                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_3                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_1                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_2                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_3 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_3                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,313</span> (20.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,313\u001b[0m (20.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,089</span> (19.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,089\u001b[0m (19.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration:\n",
      "- Batch size: 32\n",
      "- Max epochs: 200\n",
      "- Early stopping patience: 15\n",
      "- Learning rate reduction patience: 10\n",
      "\n",
      "Training Started...\n",
      "Epoch 1/200\n",
      "\u001b[1m203/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.5366 - accuracy: 0.5131 - loss: 0.9349 - precision: 0.0329 - recall: 0.5834\n",
      "Epoch 1: val_loss improved from None to 0.79834, saving model to ../data/processed/nba_injury_predictor_v1_best_nn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.5545 - accuracy: 0.5204 - loss: 0.8291 - precision: 0.0285 - recall: 0.5749 - val_AUC: 0.5760 - val_accuracy: 0.7713 - val_loss: 0.7983 - val_precision: 0.0364 - val_recall: 0.2597 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m193/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - AUC: 0.6263 - accuracy: 0.5467 - loss: 0.7682 - precision: 0.0401 - recall: 0.6602\n",
      "Epoch 2: val_loss improved from 0.79834 to 0.77461, saving model to ../data/processed/nba_injury_predictor_v1_best_nn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6371 - accuracy: 0.5568 - loss: 0.7017 - precision: 0.0344 - recall: 0.6467 - val_AUC: 0.6141 - val_accuracy: 0.7425 - val_loss: 0.7746 - val_precision: 0.0521 - val_recall: 0.4416 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m193/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6650 - accuracy: 0.5943 - loss: 0.7400 - precision: 0.0466 - recall: 0.6958\n",
      "Epoch 3: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.6527 - accuracy: 0.5950 - loss: 0.6918 - precision: 0.0376 - recall: 0.6467 - val_AUC: 0.6053 - val_accuracy: 0.7164 - val_loss: 0.7788 - val_precision: 0.0435 - val_recall: 0.4026 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m212/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6549 - accuracy: 0.6014 - loss: 0.7417 - precision: 0.0433 - recall: 0.6452\n",
      "Epoch 4: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.6800 - accuracy: 0.6047 - loss: 0.6595 - precision: 0.0405 - recall: 0.6826 - val_AUC: 0.5871 - val_accuracy: 0.7363 - val_loss: 0.7988 - val_precision: 0.0441 - val_recall: 0.3766 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m201/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.6492 - accuracy: 0.6099 - loss: 0.7494 - precision: 0.0439 - recall: 0.6226\n",
      "Epoch 5: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.6569 - accuracy: 0.6171 - loss: 0.6793 - precision: 0.0370 - recall: 0.5988 - val_AUC: 0.5904 - val_accuracy: 0.7250 - val_loss: 0.7913 - val_precision: 0.0449 - val_recall: 0.4026 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m196/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7223 - accuracy: 0.6384 - loss: 0.6714 - precision: 0.0509 - recall: 0.6779\n",
      "Epoch 6: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7122 - accuracy: 0.6467 - loss: 0.6316 - precision: 0.0441 - recall: 0.6647 - val_AUC: 0.5895 - val_accuracy: 0.7464 - val_loss: 0.8003 - val_precision: 0.0473 - val_recall: 0.3896 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m205/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7150 - accuracy: 0.6546 - loss: 0.6923 - precision: 0.0548 - recall: 0.7061\n",
      "Epoch 7: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.7331 - accuracy: 0.6641 - loss: 0.6157 - precision: 0.0497 - recall: 0.7186 - val_AUC: 0.5958 - val_accuracy: 0.7694 - val_loss: 0.7993 - val_precision: 0.0506 - val_recall: 0.3766 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m216/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7544 - accuracy: 0.6843 - loss: 0.6345 - precision: 0.0604 - recall: 0.7117\n",
      "Epoch 8: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7632 - accuracy: 0.6842 - loss: 0.5853 - precision: 0.0531 - recall: 0.7246 - val_AUC: 0.5923 - val_accuracy: 0.7924 - val_loss: 0.8167 - val_precision: 0.0512 - val_recall: 0.3377 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m213/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7764 - accuracy: 0.6952 - loss: 0.6085 - precision: 0.0650 - recall: 0.7440\n",
      "Epoch 9: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7588 - accuracy: 0.6989 - loss: 0.5896 - precision: 0.0532 - recall: 0.6886 - val_AUC: 0.5950 - val_accuracy: 0.7935 - val_loss: 0.8176 - val_precision: 0.0533 - val_recall: 0.3506 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m203/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7874 - accuracy: 0.7032 - loss: 0.5955 - precision: 0.0644 - recall: 0.7205\n",
      "Epoch 10: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7796 - accuracy: 0.7072 - loss: 0.5653 - precision: 0.0551 - recall: 0.6946 - val_AUC: 0.6068 - val_accuracy: 0.8134 - val_loss: 0.8323 - val_precision: 0.0630 - val_recall: 0.3766 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m189/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7847 - accuracy: 0.7187 - loss: 0.6129 - precision: 0.0687 - recall: 0.7193\n",
      "Epoch 11: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.7704 - accuracy: 0.7110 - loss: 0.5769 - precision: 0.0553 - recall: 0.6886 - val_AUC: 0.6098 - val_accuracy: 0.8165 - val_loss: 0.8350 - val_precision: 0.0642 - val_recall: 0.3766 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m189/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7935 - accuracy: 0.7176 - loss: 0.5983 - precision: 0.0711 - recall: 0.7442\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8045 - accuracy: 0.7160 - loss: 0.5433 - precision: 0.0606 - recall: 0.7485 - val_AUC: 0.6003 - val_accuracy: 0.8165 - val_loss: 0.8594 - val_precision: 0.0622 - val_recall: 0.3636 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m196/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8113 - accuracy: 0.7303 - loss: 0.5766 - precision: 0.0687 - recall: 0.7037\n",
      "Epoch 13: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - AUC: 0.8184 - accuracy: 0.7299 - loss: 0.5263 - precision: 0.0627 - recall: 0.7365 - val_AUC: 0.5950 - val_accuracy: 0.8313 - val_loss: 0.8873 - val_precision: 0.0659 - val_recall: 0.3506 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m200/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.7975 - accuracy: 0.7354 - loss: 0.5962 - precision: 0.0720 - recall: 0.7259\n",
      "Epoch 14: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8071 - accuracy: 0.7306 - loss: 0.5379 - precision: 0.0606 - recall: 0.7066 - val_AUC: 0.5945 - val_accuracy: 0.8247 - val_loss: 0.8808 - val_precision: 0.0673 - val_recall: 0.3766 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m202/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8246 - accuracy: 0.7344 - loss: 0.5568 - precision: 0.0781 - recall: 0.7921\n",
      "Epoch 15: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8147 - accuracy: 0.7310 - loss: 0.5277 - precision: 0.0643 - recall: 0.7545 - val_AUC: 0.5980 - val_accuracy: 0.8270 - val_loss: 0.8842 - val_precision: 0.0641 - val_recall: 0.3506 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m205/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8259 - accuracy: 0.7289 - loss: 0.5516 - precision: 0.0750 - recall: 0.7813\n",
      "Epoch 16: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8179 - accuracy: 0.7290 - loss: 0.5243 - precision: 0.0656 - recall: 0.7784 - val_AUC: 0.6033 - val_accuracy: 0.8239 - val_loss: 0.8861 - val_precision: 0.0650 - val_recall: 0.3636 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m207/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - AUC: 0.8419 - accuracy: 0.7466 - loss: 0.5250 - precision: 0.0847 - recall: 0.8235\n",
      "Epoch 17: val_loss did not improve from 0.77461\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - AUC: 0.8171 - accuracy: 0.7388 - loss: 0.5259 - precision: 0.0670 - recall: 0.7665 - val_AUC: 0.6072 - val_accuracy: 0.8274 - val_loss: 0.8868 - val_precision: 0.0622 - val_recall: 0.3377 - learning_rate: 5.0000e-04\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Training completed in 11.9 seconds (0.2 minutes)\n",
      "\n",
      "NEURAL NETWORK TRAINING Performance:\n",
      "- Accuracy: 0.771\n",
      "- Precision: 0.059\n",
      "- Recall: 0.569\n",
      "- F1-Score: 0.106\n",
      "- ROC AUC: 0.737\n",
      "- PR AUC: 0.077\n",
      "Confusion Matrix:\n",
      "    TN: 5281 | FP: 1527\n",
      "    FN:   72 | TP:   95\n",
      "\n",
      "NEURAL NETWORK VALIDATION Performance:\n",
      "- Accuracy: 0.743\n",
      "- Precision: 0.052\n",
      "- Recall: 0.442\n",
      "- F1-Score: 0.093\n",
      "- ROC AUC: 0.615\n",
      "- PR AUC: 0.055\n",
      "Confusion Matrix:\n",
      "    TN: 1872 | FP:  618\n",
      "    FN:   43 | TP:   34\n",
      "\n",
      "NEURAL NETWORK TEST Performance:\n",
      "- Accuracy: 0.616\n",
      "- Precision: 0.009\n",
      "- Recall: 0.333\n",
      "- F1-Score: 0.017\n",
      "- ROC AUC: 0.449\n",
      "- PR AUC: 0.013\n",
      "Confusion Matrix:\n",
      "    TN:  361 | FP:  222\n",
      "    FN:    4 | TP:    2\n",
      "Top K Risk Prediction Performance:\n",
      "    Top  5%:  11.7% of injuries captured, precision =   7.0%\n",
      "    Top 10%:  20.8% of injuries captured, precision =   6.2%\n",
      "    Top 15%:  26.0% of injuries captured, precision =   5.2%\n",
      "    Top 20%:  36.4% of injuries captured, precision =   5.5%\n",
      "Training History Summary:\n",
      "- Total epochs trained: 17\n",
      "- Best epoch (lowest val_loss): 2\n",
      "- Final training loss: 0.5259\n",
      "- Final validation loss: 0.8868\n",
      "- Best validation loss: 0.7746\n",
      "\n",
      "Model Complexity:\n",
      "- Total parameters: 5,313\n",
      "- Trainable parameters: 5,089\n",
      "- Model size: ~20.8 KB (float32)\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Architecture\n",
    "def create_neural_network(input_dim, class_weights):\n",
    "    \"\"\"\n",
    "    Creates neural network architecture\n",
    "    - Input: 40 features  \n",
    "    - Hidden Layer 1: 64 units, ReLU, Dropout(0.3)\n",
    "    - Hidden Layer 2: 32 units, ReLU, Dropout(0.3) \n",
    "    - Hidden Layer 3: 16 units, ReLU, Dropout(0.2)\n",
    "    - Output: 1 unit, Sigmoid\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,), name='hidden_1'),\n",
    "        Dropout(0.3, name='dropout_1'),\n",
    "        BatchNormalization(name='batch_norm_1'),\n",
    "        \n",
    "        # 2nd hidden layer\n",
    "        Dense(32, activation='relu', name='hidden_2'),\n",
    "        Dropout(0.3, name='dropout_2'),\n",
    "        BatchNormalization(name='batch_norm_2'),\n",
    "        \n",
    "        # 3rd hidden layer  \n",
    "        Dense(16, activation='relu', name='hidden_3'),\n",
    "        Dropout(0.2, name='dropout_3'),\n",
    "        BatchNormalization(name='batch_norm_3'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    # Compiles w/ class weights incorporated into loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall', 'AUC']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Convert class weights to sample weights for TensorFlow\n",
    "def create_sample_weights(y, class_weights):\n",
    "    \"\"\"Convert class weights to sample weights for TensorFlow training\"\"\"\n",
    "    sample_weights = np.where(y == 1, class_weights[1], class_weights[0])\n",
    "    return sample_weights\n",
    "\n",
    "# Training setup\n",
    "print(\"Setting up Neural Network training...\")\n",
    "print(f\"Input features: {X_train_tf.shape[1]}\")\n",
    "print(f\"Training samples: {X_train_tf.shape[0]:,}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Creates sample weights for training\n",
    "train_sample_weights = create_sample_weights(y_train_tf, class_weights)\n",
    "val_sample_weights = create_sample_weights(y_val_tf, class_weights)\n",
    "\n",
    "print(f\"Sample weights created. Shape: {train_sample_weights.shape}\")\n",
    "print(f\"Sample weight distribution: {np.unique(train_sample_weights, return_counts=True)}\")\n",
    "\n",
    "# Builds the model\n",
    "nn_model = create_neural_network(X_train_tf.shape[1], class_weights)\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nNeural Network Architecture:\")\n",
    "nn_model.summary()\n",
    "\n",
    "# Callbacks for training\n",
    "callbacks_list = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'../data/processed/{MODEL_NAME}_best_nn.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "VALIDATION_SPLIT = 0.0  # Separate validation set\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")\n",
    "print(f\"- Max epochs: {EPOCHS}\")\n",
    "print(f\"- Early stopping patience: 15\")\n",
    "print(f\"- Learning rate reduction patience: 10\")\n",
    "\n",
    "# Trains the model\n",
    "print(\"\\nTraining Started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = nn_model.fit(\n",
    "    X_train_tf, y_train_tf,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_tf, y_val_tf, val_sample_weights),\n",
    "    sample_weight=train_sample_weights,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "\n",
    "# Training predictions\n",
    "y_train_prob_nn = nn_model.predict(X_train_tf, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_train_pred_nn = (y_train_prob_nn > 0.5).astype(int)\n",
    "\n",
    "# Validation predictions  \n",
    "y_val_prob_nn = nn_model.predict(X_val_tf, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_val_pred_nn = (y_val_prob_nn > 0.5).astype(int)\n",
    "\n",
    "# Test predictions\n",
    "y_test_prob_nn = nn_model.predict(X_test_tf, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_test_pred_nn = (y_test_prob_nn > 0.5).astype(int)\n",
    "\n",
    "# Evaluations\n",
    "nn_train_metrics = evaluate_model_performance(y_train_tf.astype(int), y_train_pred_nn, y_train_prob_nn, \"Neural Network Training\")\n",
    "nn_val_metrics = evaluate_model_performance(y_val_tf.astype(int), y_val_pred_nn, y_val_prob_nn, \"Neural Network Validation\") \n",
    "nn_test_metrics = evaluate_model_performance(y_test_tf.astype(int), y_test_pred_nn, y_test_prob_nn, \"Neural Network Test\")\n",
    "\n",
    "# Top K Risk Analysis for Neural Network\n",
    "analyze_top_k_predictions(y_val_tf.astype(int), y_val_prob_nn)\n",
    "\n",
    "# Training history summary\n",
    "print(\"Training History Summary:\")\n",
    "print(f\"- Total epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"- Best epoch (lowest val_loss): {np.argmin(history.history['val_loss']) + 1}\")\n",
    "print(f\"- Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"- Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"- Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if 'val_auc' in history.history:\n",
    "    print(f\"- Final validation AUC: {history.history['val_auc'][-1]:.4f}\")\n",
    "    print(f\"- Best validation AUC: {max(history.history['val_auc']):.4f}\")\n",
    "\n",
    "# Model complexity summary\n",
    "total_params = nn_model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in nn_model.trainable_weights])\n",
    "\n",
    "print(f\"\\nModel Complexity:\")\n",
    "print(f\"- Total parameters: {total_params:,}\")\n",
    "print(f\"- Trainable parameters: {trainable_params:,}\") \n",
    "print(f\"- Model size: ~{total_params * 4 / 1024:.1f} KB (float32)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
